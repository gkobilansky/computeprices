id,name,vram,created_at,manufacturer,link,architecture,use_cases,description,slug,image_url,compute_units,cuda_cores,tensor_cores,rt_cores,memory_bandwidth_gbps,memory_interface_bit,manufacturing_process_nm,tdp_watt,max_power_watt,release_date,end_of_life_date,fp16_performance_tflops,fp32_performance_tflops,fp64_performance_tflops,int8_performance_tops,ml_perf_inference_score,ml_perf_training_score,msrp_usd,server_gpu,cloud_compatible,detailed_description,pros,cons,features,benchmark_links,affiliate_links,performance_tier,generation
4f72c25a-0799-4d27-a2ac-8cdb828058fd,A10,24.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/data-center/products/a10-gpu/,Ampere,"AI inference, rendering",,a10,,,9216.0,288.0,72.0,600.0,384.0,8.0,150.0,150.0,2021,,125.0,31.2,,250.0,,,,False,True,,,,,,,,
0549ed49-7227-452e-a31d-a29941c5eac2,A100 PCIE,40.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/data-center/a100/,Ampere,"AI training, HPC",,a100pcie,,,6912.0,432.0,0.0,1555.0,5120.0,7.0,250.0,250.0,"Jun 22nd, 2020",,312.0,19.5,9.7,624.0,,,,False,True,,,,,,,,
18ae1548-0a72-4e0c-82eb-8b90b4c841ad,A100 SXM,80.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/data-center/a100/,Ampere,"Large-scale AI training, HPC",,a100sxm,,,6912.0,432.0,0.0,2039.0,5120.0,7.0,400.0,400.0,"Nov 16th, 2020",,312.0,19.5,9.7,624.0,,,,False,True,,,,,,,,
6cad9dd7-5e51-4b5c-ac10-835f8be14d83,A40,48.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/data-center/a40/,Ampere,"Data center visual computing, AI",,a40,,,10752.0,336.0,84.0,696.0,384.0,8.0,300.0,300.0,2020,,149.7,37.4,,299.3,,,,False,True,,,,,,,,
cf874b6e-7f9c-4c06-9bf6-37be87eeac9a,B200,192.0,2025-04-01 01:43:51.687075+00,NVIDIA,https://www.nvidia.com/en-us/data-center/dgx-b200/,Blackwell,"Training Trillion-Parameter+ AI Models
Large-Scale AI Inference
High-Performance Computing
Massive Data Analytics
Generative AI Beyond Text","The B200 is pushing the boundaries of AI model scale and performance, enabling computations that were previously impractical, and doing so with potentially better total cost of ownership and energy efficiency compared to scaling out with older generations for the same task.",b200,,,,,,8000.0,,,1000.0,1000.0,2024,,4500.0,80.0,40.0,9000.0,,,,False,True,,,,,,,,
22218f48-44f7-4902-b1fe-560b09c9b8eb,GH200,96.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/,Hopper,"AI/ML, HPC with large datasets",,gh200,,,16896.0,528.0,,4000.0,,4.0,700.0,1000.0,2023,,990.0,67.0,34.0,1979.0,,,,False,True,,,,,,,,
bd29088a-58de-4523-a1ec-b9e79ac084a6,H100,80.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/data-center/h100/,Hopper,"The H100 GPU is designed for the most demanding AI and HPC workloads, including training and inference for large language models (LLMs), foundation models, generative AI, healthcare research, scientific simulation, and financial modeling. It excels at large-scale machine learning tasks, natural language processing, computer vision, and computational chemistry.","NVIDIA H100 Tensor Core GPU, the most advanced accelerator for AI and HPC workloads",h100,/images/gpus/h100.jpg,132.0,14592.0,456.0,,2000.0,,4.0,350.0,350.0,2022,,756.0,51.0,26.0,1513.0,940.0,845.5,30000.0,True,True,"The NVIDIA H100 Tensor Core GPU is built on the groundbreaking NVIDIA Hopper architecture to deliver the next massive leap in accelerated computing. H100 is designed to handle the most compute-intensive workloads, including foundation models, large language models, recommender systems, and scientific simulations. It features fourth-generation Tensor Cores and Transformer Engine for unprecedented AI performance, and introduces new features like Dynamic Programming Accelerator for dynamic programming algorithms.","[""Groundbreaking performance for AI workloads"",""Next-generation Tensor Cores with significant performance uplift"",""Transformer Engine for accelerating language models"",""New architectural features for advanced computing tasks"",""Support for HBM3 memory with massive bandwidth""]","[""Very high power consumption"",""Premium pricing"",""Requires specialized cooling and power infrastructure"",""Not suitable for most consumer or small business applications""]","[""Fourth-generation Tensor Cores"",""Transformer Engine"",""Dynamic Programming Accelerator"",""HBM3 Memory"",""NVLink 4.0"",""PCIe Gen 5"",""Confidential Computing"",""Multi-Instance GPU (MIG) with up to 7 instances""]","{""MLPerf Training Results"": ""https://mlcommons.org/en/training-normal-20/"", ""MLPerf Inference Results"": ""https://mlcommons.org/en/inference-datacenter-55/""}","{""AWS"": ""https://aws.amazon.com/ec2/instance-types/p5/"", ""NVIDIA"": ""https://www.nvidia.com/en-us/data-center/h100/"", ""Google Cloud"": ""https://cloud.google.com/compute/docs/gpus""}",ultra,9.0
242d0457-c655-46c9-98cc-5406500ebcdb,H200,141.0,2024-12-23 14:54:10.94787+00,NVIDIA,https://www.nvidia.com/en-us/data-center/h200/,Hopper,Generative AI and large language models (LLMs) training. Advanced scientific computing for HPC workloads.,,h200,,,16896.0,528.0,,4800.0,,4.0,700.0,700.0,2023,,990.0,67.0,34.0,1979.0,,,,False,True,,,,,,,,
35ea1053-61bd-4faa-96da-39fde253a040,L40,40.0,2025-01-22 19:31:23.488922+00,NVIDIA,https://www.nvidia.com/en-us/data-center/l40/,Ada Lovelace,"neural graphics, virtualization, compute, AI",,l40,,,18176.0,568.0,142.0,864.0,384.0,4.0,300.0,300.0,"Oct 13th, 2022",,181.05,90.5,1.414,362.0,,,,False,True,,,,,,,,
b6bdde1e-9b0e-4a64-8579-ac34e7da1d6f,L40S,48.0,2025-02-15 00:41:24.390065+00,NVIDIA,https://www.nvidia.com/en-us/data-center/l40s/,Ada Lovelace,"generative AI,  model training and inference, rendering, metaverse apps","The Nvidia L40S provides multi-workload acceleration for large language model (LLM) inference and training, graphics, and video applications. Based on the latest Ada Lovelace architecture.",l40s,,,18176.0,568.0,142.0,864.0,384.0,4.0,350.0,350.0,2023,,362.05,91.6,,733.0,,,,False,True,,,,,,,,
ea52929c-d3f4-4027-a97a-e174b294a784,MI300X,192.0,2025-02-15 00:30:47.666563+00,AMD,https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html,CDNA 3,AI and HPC applications. Large scale training tasks.,"First introduced in 2024, the AMD MI300X merges CPU and GPU functionalities under a unified memory system, streamlining AI and HPC workloads. AMD emphasizes its enhanced efficiency for large-scale training tasks, while also highlighting the platformâ€™s versatility across diverse workloads.",mi300x,,304.0,,,,5300.0,,5.0,750.0,750.0,2023,,1305.0,163.4,81.7,2610.0,,,,False,True,,,,,,,,
f82875ca-e3e8-4742-9233-82b1203b99a5,RTX 3090,24.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090-3090ti/,Ampere,"Deep learning, data science",,rtx3090,,,10496.0,328.0,82.0,936.2,384.0,8.0,350.0,350.0,Sep 2020,,,35.58,0.5559375,,,,,False,True,,,,,,,,
65705086-e514-4584-95f6-39d2252455be,RTX 4080,16.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4080-family/,Ada Lovelace,"Professional graphics, VR applications",,rtx4080,,,9728.0,304.0,76.0,716.8,256.0,5.0,320.0,320.0,"Nov 20, 2022",,,48.834559999999996,0.7630399999999999,,,,,False,True,,,,,,,,
689b878b-64ad-4cd3-8480-ef53ea39d665,RTX 4090,24.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/,Ada Lovelace,"High-end AI/ML workloads, 3D rendering",,rtx4090,,,16384.0,512.0,128.0,1008.0,384.0,5.0,450.0,450.0,"Oct 12th, 2022",,,82.6,1.290625,,,,,False,True,,,,,,,,
72fd4577-5489-424d-8a88-25b867b83fc2,RTX 6000 Ada,48.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/design-visualization/rtx-6000/,Ada Lovelace,"Professional visualization, AI",,rtx6000ada,,,18176.0,568.0,142.0,960.0,384.0,4.0,300.0,300.0,2022,,728.5,91.1,1.4234375,728.5,,,,False,True,,,,,,,,
8a657d8d-720b-4532-8422-30cc9ac6e588,RTX A4000,16.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/design-visualization/rtx-a4000/,Ampere,"Professional visualization, AI",,rtxa4000,,,6144.0,192.0,48.0,448.0,256.0,8.0,140.0,140.0,"August 15, 2021",,76.7,19.2,0.3,153.4,,,,False,True,,,,,,,,
05333202-7123-48b9-a58f-8c41446e390b,RTX A5000,24.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-in/design-visualization/rtx-a5000/,Ampere,"Professional visualization, AI",,rtxa5000,,,8192.0,256.0,64.0,768.0,384.0,8.0,230.0,230.0,"Apr 12, 2021",,111.1,27.8,0.434375,222.2,,,,False,True,,,,,,,,
1149ba7f-4499-4abc-ade8-bdcbe4ba386c,RTX A6000,48.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/design-visualization/rtx-a6000/,Ampere,"Professional visualization, AI",,rtxa6000,,,10752.0,336.0,84.0,768.0,384.0,8.0,300.0,300.0,2020,,154.85,38.7,0.6046875,309.7,,,,False,True,,,,,,,,
0930daf2-de70-4494-8beb-1933377652b4,Tesla T4,16.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-us/data-center/tesla-t4/,Turing,"Inference, video transcoding",,t4,,,2560.0,320.0,40.0,320.0,256.0,12.0,70.0,70.0,"September 13th, 2018",,65.0,8.1,0.253125,130.0,,,,False,True,,,,,,,,
dfa19b27-8663-43ad-8d35-8ed5a2569ae3,Tesla V100,32.0,2024-12-19 19:45:00.592604+00,NVIDIA,https://www.nvidia.com/en-gb/data-center/tesla-v100/,Volta,"AI training, HPC",,v100,,,5120.0,640.0,,900.0,4096.0,12.0,250.0,250.0,2017,,112.0,14.0,7.0,,,,,False,True,,,,,,,,
