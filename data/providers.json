[
  {
    "id": "3bb5a379-472f-4c84-9ba4-3337f3922582",
    "name": "Amazon AWS",
    "slug": "aws",
    "description": "AWS provides a comprehensive suite of cloud computing services, including compute, storage, and GPU solutions for diverse workloads.",
    "link": "https://aws.amazon.com",
    "docsLink": "https://docs.aws.amazon.com",
    "features": [
      {
        "title": "Global Infrastructure",
        "description": "Extensive network of data centers across multiple regions worldwide"
      },
      {
        "title": "Pay-as-you-go Pricing",
        "description": "Flexible pricing model with no upfront commitments required"
      },
      {
        "title": "Advanced Security",
        "description": "Comprehensive security tools and compliance certifications"
      },
      {
        "title": "Auto Scaling",
        "description": "Automatically adjust resources based on demand"
      },
      {
        "title": "Integrated Services",
        "description": "Extensive ecosystem of services that work seamlessly together"
      },
      {
        "title": "Developer Tools",
        "description": "Comprehensive suite of tools for development, deployment, and management"
      }
    ],
    "pros": [
      "Broad range of compute options including GPUs",
      "Highly scalable and reliable infrastructure",
      "Pay-as-you-go pricing with cost optimization tools",
      "Extensive global network of data centers",
      "Rich ecosystem of integrated services and tools"
    ],
    "cons": [
      "Complex pricing structure",
      "Steep learning curve for new users",
      "Potential for unexpected costs without proper management"
    ],
    "gettingStarted": [
      {
        "title": "Sign up for AWS",
        "description": "Create an AWS account to access the cloud platform."
      },
      {
        "title": "Choose a compute service",
        "description": "Select from EC2, Lambda, or container services based on your workload needs."
      },
      {
        "title": "Launch an instance",
        "description": "Configure and launch your first compute instance or container."
      },
      {
        "title": "Set up security",
        "description": "Configure security groups and access controls for your resources."
      },
      {
        "title": "Monitor and optimize",
        "description": "Use AWS CloudWatch and Compute Optimizer to monitor performance and reduce costs."
      }
    ],
    "computeServices": [
      {
        "name": "Amazon EC2",
        "description": "Virtual servers in the cloud with a wide range of instance types.",
        "instanceTypes": [
          {
            "name": "P4 Instances",
            "description": "Powered by NVIDIA A100 Tensor Core GPUs, optimized for AI and HPC workloads.",
            "features": [
              "Up to 8 NVIDIA A100 GPUs per instance",
              "600 GB/s GPU-to-GPU bandwidth with NVSwitch",
              "Elastic Fabric Adapter (EFA) for low-latency, multi-node GPU training",
              "Network throughput of up to 400 Gbps"
            ]
          },
          {
            "name": "P3 Instances",
            "description": "Equipped with NVIDIA V100 Tensor Core GPUs for deep learning and HPC.",
            "features": [
              "Up to 8 NVIDIA V100 GPUs per instance",
              "300 GB/s GPU-to-GPU bandwidth with NVLink",
              "Suitable for FP16 and FP32 precision workloads"
            ]
          },
          {
            "name": "G5 Instances",
            "description": "Powered by NVIDIA A10G GPUs for graphics-intensive applications.",
            "features": [
              "Optimized for real-time ray tracing and AI-driven graphics",
              "Suitable for game streaming and content creation"
            ]
          },
          {
            "name": "P5 Instances",
            "description": "Powered by NVIDIA H100 Tensor Core GPUs for next\u2011gen AI and HPC.",
            "features": [
              "Up to 8 NVIDIA H100 GPUs per instance",
              "Amazon EC2 UltraClusters scale to tens of thousands of H100 GPUs",
              "Up to 3,200 Gbps aggregate networking with EFA and GPUDirect RDMA"
            ]
          },
          {
            "name": "G6 Instances",
            "description": "Powered by NVIDIA L4 Tensor Core GPUs for graphics, video, and inference.",
            "features": [
              "Up to 8 NVIDIA L4 GPUs per instance (24 GB per GPU)",
              "Fractional GPU sizes down to 1/8 of an L4",
              "Up to 192 vCPUs and 7.5 TB local NVMe"
            ]
          },
          {
            "name": "G6e Instances",
            "description": "Powered by NVIDIA L40S GPUs for advanced AI inference and 3D workloads.",
            "features": [
              "Up to 8 NVIDIA L40S GPUs per instance (48 GB per GPU)",
              "Up to 400 Gbps networking and 1.5 TB system memory",
              "Ideal for LLM serving, diffusion, and digital twins"
            ]
          }
        ]
      },
      {
        "name": "Amazon ECS",
        "description": "Fully managed container orchestration service.",
        "features": [
          "Support for Docker containers",
          "Integration with other AWS services",
          "Automated cluster management and scheduling"
        ]
      },
      {
        "name": "Amazon EKS",
        "description": "Managed Kubernetes service for container orchestration.",
        "features": [
          "Certified Kubernetes conformant",
          "Integrates with AWS networking and security services",
          "Supports both EC2 and Fargate launch types"
        ]
      },
      {
        "name": "AWS Lambda",
        "description": "Serverless compute service for running code without managing servers.",
        "features": [
          "Automatic scaling and high availability",
          "Pay only for compute time consumed",
          "Supports multiple programming languages"
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "EC2 GPU Instances",
        "description": "EC2 instances equipped with powerful GPUs for compute-intensive workloads.",
        "types": [
          {
            "name": "P4 Instances",
            "gpuModel": "NVIDIA A100 Tensor Core",
            "bestFor": "Large-scale AI training, HPC simulations"
          },
          {
            "name": "P3 Instances",
            "gpuModel": "NVIDIA V100 Tensor Core",
            "bestFor": "Deep learning training and inference, scientific simulations"
          },
          {
            "name": "G5 Instances",
            "gpuModel": "NVIDIA A10G",
            "bestFor": "Graphics rendering, game streaming, machine learning inference"
          },
          {
            "name": "P5 Instances",
            "gpuModel": "NVIDIA H100 Tensor Core",
            "bestFor": "Frontier\u2011scale training; distributed HPC"
          },
          {
            "name": "G6 Instances",
            "gpuModel": "NVIDIA L4",
            "bestFor": "Video, graphics, and economical inference"
          },
          {
            "name": "G6e Instances",
            "gpuModel": "NVIDIA L40S",
            "bestFor": "High\u2011throughput inference and 3D/digital twin workloads"
          }
        ]
      },
      {
        "name": "Amazon SageMaker",
        "description": "Fully managed machine learning platform with GPU support.",
        "features": [
          "Integrated Jupyter notebooks with GPU acceleration",
          "Automated model tuning and deployment",
          "Built-in algorithms optimized for GPU execution"
        ]
      }
    ],
    "pricingOptions": [
      {
        "name": "On-Demand Instances",
        "description": "Pay for compute capacity by the second with no long-term commitments."
      },
      {
        "name": "Spot Instances",
        "description": "Use spare EC2 capacity at up to 90% off the On-Demand price."
      },
      {
        "name": "Reserved Instances",
        "description": "Save up to 72% compared to On-Demand pricing with a 1 or 3-year commitment."
      },
      {
        "name": "Savings Plans",
        "description": "Save up to 72% on compute usage with a 1 or 3-year commitment to a consistent amount of usage."
      }
    ],
    "regions": "30+ regions and 100+ availability zones worldwide.",
    "support": "Basic (free), Developer, Business, Enterprise support plans with varying response times and features. Extensive documentation, forums, and training resources."
  },
  {
    "id": "30a69dae-5939-499a-a4f5-5114797dcdb3",
    "name": "RunPod",
    "slug": "runpod",
    "description": "RunPod offers on\u2011demand GPUs and instant multi\u2011node clusters across 30+ regions, with H100/H200 alongside A100, L40S, and RTX classes.",
    "link": "https://runpod.io",
    "docsLink": "https://docs.runpod.io",
    "features": [
      {
        "title": "Secure Cloud GPUs",
        "description": "Access to a wide range of GPU types with enterprise-grade security"
      },
      {
        "title": "Pay-as-you-go",
        "description": "Only pay for the compute time you actually use"
      },
      {
        "title": "API Access",
        "description": "Programmatically manage your GPU instances via REST API"
      },
      {
        "title": "Fast cold-starts",
        "description": "Pods typically ready in 20-30 s"
      },
      {
        "title": "Hot-reload dev loop",
        "description": "SSH & VS Code tunnels built-in"
      },
      {
        "title": "Spot-to-on-demand fallback",
        "description": "Automatic migration on pre-empt"
      }
    ],
    "pros": [
      "Competitive pricing with pay-per-second billing",
      "Wide variety of GPU options",
      "Simple and intuitive interface"
    ],
    "cons": [
      "GPU availability can vary by region",
      "Some features require technical knowledge"
    ],
    "gettingStarted": [
      {
        "title": "Create an account",
        "description": "Sign up for RunPod using your email or GitHub account"
      },
      {
        "title": "Add payment method",
        "description": "Add a credit card or cryptocurrency payment method"
      },
      {
        "title": "Launch your first pod",
        "description": "Select a template and GPU type to launch your first instance"
      }
    ],
    "computeServices": [
      {
        "name": "Pods",
        "description": "On\u2011demand single\u2011node GPU instances with flexible templates and storage.",
        "instanceTypes": [
          {
            "name": "H100 (SXM & PCIe)",
            "description": "80 GB HBM; options for SXM/NVLink or PCIe; popular for LLM training/inference.",
            "features": [
              "Minute billing",
              "Templates for fine\u2011tune/inference",
              "Marketplace images"
            ]
          },
          {
            "name": "H200",
            "description": "141 GB HBM3e; higher bandwidth for larger context windows.",
            "features": [
              "On\u2011demand where available",
              "Great for high\u2011throughput serving"
            ]
          }
        ]
      },
      {
        "name": "Instant Clusters",
        "description": "Spin up multi\u2011node GPU clusters in minutes with auto networking.",
        "instanceTypes": [
          {
            "name": "H100 Clusters",
            "description": "Scale from a few to hundreds of H100 GPUs.",
            "features": [
              "Preset topologies",
              "EFA\u2011like low\u2011latency fabrics (provider managed)",
              "K8s ready images"
            ]
          }
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "GPU Catalog",
        "description": "Catalog of available GPU types and VRAM classes.",
        "types": [
          {
            "name": "H100",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training & inference; SXM or PCIe"
          },
          {
            "name": "H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Larger tokens/context; faster inference"
          },
          {
            "name": "A100",
            "gpuModel": "NVIDIA A100 80GB",
            "bestFor": "Balanced price/perf"
          },
          {
            "name": "L40S",
            "gpuModel": "NVIDIA L40S",
            "bestFor": "Vision/video & fast inference"
          }
        ]
      }
    ]
  },
  {
    "id": "d122f315-b060-4924-8726-788c28ed3905",
    "name": "Google Cloud",
    "slug": "google",
    "description": "GCP provides powerful GPU instances with flexible pricing and integration with Google's AI and machine learning tools. It's a major cloud provider known for its innovation in Kubernetes, AI/ML, and data analytics.",
    "link": "https://cloud.google.com/gpu",
    "docsLink": "https://cloud.google.com/compute/docs",
    "features": [
      {
        "title": "Compute Engine",
        "description": "Scalable virtual machines with a wide range of machine types, including GPUs."
      },
      {
        "title": "Google Kubernetes Engine (GKE)",
        "description": "Managed Kubernetes service for deploying and managing containerized applications."
      },
      {
        "title": "Cloud Functions",
        "description": "Event-driven serverless compute platform."
      },
      {
        "title": "Cloud Run",
        "description": "Fully managed serverless platform for containerized applications."
      },
      {
        "title": "Vertex AI",
        "description": "Unified ML platform for building, deploying, and managing ML models."
      },
      {
        "title": "Preemptible VMs",
        "description": "Short-lived compute instances at a significant discount, suitable for fault-tolerant workloads."
      },
      {
        "title": "Cloud Storage",
        "description": "Scalable and durable object storage."
      },
      {
        "title": "Persistent Disk",
        "description": "Block storage for Compute Engine instances."
      },
      {
        "title": "Cloud Load Balancing",
        "description": "High-performance, scalable load balancing."
      },
      {
        "title": "Virtual Private Cloud (VPC)",
        "description": "Software-defined networking for your cloud resources."
      }
    ],
    "pros": [
      "Flexible pricing options, including sustained use discounts",
      "Strong AI and machine learning tools (Vertex AI)",
      "Good integration with other Google services",
      "Cutting-edge Kubernetes implementation (GKE)",
      "Competitive pricing, especially for sustained use",
      "Strong global network infrastructure",
      "Innovative AI/ML and data analytics services"
    ],
    "cons": [
      "Limited availability in some regions compared to AWS",
      "Complexity in managing resources",
      "Support can be costly",
      "Steeper learning curve for some services"
    ],
    "gettingStarted": [
      {
        "title": "Create a Google Cloud project",
        "description": "Set up a project in the Google Cloud Console."
      },
      {
        "title": "Enable billing",
        "description": "Set up a billing account to pay for resource usage."
      },
      {
        "title": "Choose a compute service",
        "description": "Select Compute Engine, GKE, Cloud Functions, or Cloud Run based on your needs."
      },
      {
        "title": "Create and configure an instance",
        "description": "Launch a VM instance, configure a Kubernetes cluster, or deploy a function/application."
      },
      {
        "title": "Manage resources",
        "description": "Use the Cloud Console, command-line tools, or APIs to manage your resources."
      }
    ],
    "computeServices": [
      {
        "name": "Compute Engine",
        "description": "Offers customizable virtual machines running in Google's data centers.",
        "instanceTypes": [
          {
            "name": "A2",
            "description": "Based on NVIDIA A100 Tensor Core GPUs, ideal for demanding AI/ML and HPC workloads.",
            "features": [
              "Up to 16 NVIDIA A100 GPUs per instance",
              "High-speed NVLink interconnect between GPUs",
              "Suitable for large-scale model training and inference"
            ]
          },
          {
            "name": "G2",
            "description": "Powered by NVIDIA L4 GPUs, optimized for graphics-intensive applications and inference.",
            "features": [
              "Uses the NVIDIA Ada Lovelace architecture",
              "Good for graphics rendering, game streaming, and machine learning inference"
            ]
          },
          {
            "name": "T4",
            "description": "Based on NVIDIA T4 Tensor Core GPUs, suitable for a wide range of workloads, including machine learning inference and video transcoding",
            "features": [
              "Versatile performance for various applications",
              "Cost-effective for inference workloads"
            ]
          },
          {
            "name": "V100",
            "description": "Powered by NVIDIA V100 Tensor Core GPUs, designed for high-performance deep learning training and HPC.",
            "features": [
              "High performance for FP32 and FP16 workloads",
              "Suitable for complex simulations and model training"
            ]
          },
          {
            "name": "P4",
            "description": "Equipped with NVIDIA P4 GPUs, optimized for inference workloads.",
            "features": [
              "Energy-efficient for inference",
              "Good for real-time processing and video analysis"
            ]
          },
          {
            "name": "P100",
            "description": "Based on NVIDIA P100 GPUs, suitable for a variety of HPC and deep learning applications.",
            "features": [
              "Good balance of performance and cost",
              "Suitable for scientific computing and machine learning training"
            ]
          },
          {
            "name": "K80",
            "description": "Powered by NVIDIA K80 GPUs, a cost-effective option for less demanding GPU computing tasks.",
            "features": [
              "Entry-level GPU for basic deep learning and HPC workloads",
              "Lower cost compared to newer generation GPUs"
            ]
          },
          {
            "name": "A3 High",
            "description": "H100 SXM\u2011based VMs for training and serving.",
            "features": [
              "8x NVIDIA H100 80GB SXM",
              "NVIDIA NVSwitch fabric",
              "Multiple machine sizes (1,2,4,8 GPUs)"
            ]
          },
          {
            "name": "A3 Mega",
            "description": "H100 SXM\u2011based VMs optimized for the largest training jobs.",
            "features": [
              "8x NVIDIA H100 80GB Mega GPUs",
              "High\u2011bandwidth NVLink/NVSwitch",
              "Designed for large\u2011scale LLM training"
            ]
          }
        ]
      },
      {
        "name": "Google Kubernetes Engine (GKE)",
        "description": "Managed Kubernetes service for running containerized applications.",
        "features": [
          "Automated Kubernetes operations",
          "Integration with Google Cloud services",
          "Advanced cluster management features"
        ]
      },
      {
        "name": "Cloud Functions",
        "description": "Serverless compute platform for running code in response to events.",
        "features": [
          "Automatic scaling and high availability",
          "Pay only for the compute time consumed",
          "Supports multiple programming languages"
        ]
      },
      {
        "name": "Cloud Run",
        "description": "Fully managed serverless platform for deploying and scaling containerized applications.",
        "features": [
          "Runs stateless containers on a fully managed environment",
          "Automatic scaling and high availability",
          "Pay only for the resources used"
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "Compute Engine GPU Instances",
        "description": "Virtual machines with attached GPUs for accelerated computing.",
        "types": [
          {
            "name": "A2",
            "gpuModel": "NVIDIA A100",
            "bestFor": "Large-scale AI training, HPC"
          },
          {
            "name": "G2",
            "gpuModel": "NVIDIA L4",
            "bestFor": "Graphics rendering, machine learning inference"
          },
          {
            "name": "T4",
            "gpuModel": "NVIDIA T4",
            "bestFor": "Machine learning inference, video transcoding"
          },
          {
            "name": "V100",
            "gpuModel": "NVIDIA V100",
            "bestFor": "Deep learning training, HPC"
          },
          {
            "name": "P4",
            "gpuModel": "NVIDIA P4",
            "bestFor": "Inference workloads, real-time processing"
          },
          {
            "name": "P100",
            "gpuModel": "NVIDIA P100",
            "bestFor": "HPC, deep learning training"
          },
          {
            "name": "K80",
            "gpuModel": "NVIDIA K80",
            "bestFor": "Entry-level deep learning, HPC"
          },
          {
            "name": "A3 High",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training & serving of large models"
          },
          {
            "name": "A3 Mega",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Frontier\u2011scale training"
          }
        ]
      },
      {
        "name": "Vertex AI",
        "description": "Unified machine learning platform for building, deploying, and managing ML models.",
        "features": [
          "Support for various ML frameworks",
          "Automated machine learning (AutoML)",
          "Tools for data preparation, model training, and deployment"
        ]
      }
    ],
    "pricingOptions": [
      {
        "name": "On-Demand",
        "description": "Pay for compute capacity per hour or per second, with no long-term commitments."
      },
      {
        "name": "Sustained Use Discounts",
        "description": "Automatic discounts for running instances for a significant portion of the month."
      },
      {
        "name": "Committed Use Discounts",
        "description": "Save up to 57% with a 1-year or 3-year commitment to a minimum level of resource usage."
      },
      {
        "name": "Preemptible VMs",
        "description": "Save up to 80% for fault-tolerant workloads that can be interrupted."
      }
    ],
    "regions": "40+ regions and 120+ zones worldwide.",
    "support": "Role-based (free), Standard, Enhanced and Premium support plans. Comprehensive documentation, community forums, and training resources.",
    "uniqueSellingPoints": [
      "Strong focus on Kubernetes and containerization (GKE)",
      "Cutting-edge AI and machine learning services (Vertex AI)",
      "Innovative networking infrastructure",
      "Competitive pricing, especially with sustained use discounts"
    ]
  },
  {
    "id": "11f663cd-d914-4863-9094-f293ee6421e0",
    "name": "Microsoft Azure",
    "slug": "azure",
    "description": "Azure provides comprehensive cloud computing services with strong enterprise integration, advanced AI capabilities, and a wide range of GPU options for machine learning, visualization, and high-performance computing workloads.",
    "link": "https://azure.microsoft.com/en-us/solutions/ai/",
    "docsLink": "https://learn.microsoft.com/en-us/azure/",
    "features": [
      {
        "title": "Azure AI",
        "description": "Comprehensive suite of AI services and tools for building intelligent applications"
      },
      {
        "title": "Enterprise Integration",
        "description": "Seamless integration with Microsoft ecosystem and enterprise tools"
      },
      {
        "title": "Hybrid Capabilities",
        "description": "Strong hybrid and multi-cloud support with Azure Arc"
      },
      {
        "title": "Advanced Security",
        "description": "Industry-leading security features and compliance certifications"
      },
      {
        "title": "Global Scale",
        "description": "Extensive worldwide network of data centers and edge locations"
      }
    ],
    "pros": [
      "Strong enterprise integration and support",
      "Comprehensive AI and machine learning services",
      "Advanced security and compliance features",
      "Extensive hybrid cloud capabilities",
      "Wide range of GPU options",
      "Strong .NET and Windows workload support",
      "Integrated development tools and DevOps services"
    ],
    "cons": [
      "Complex pricing and billing structure",
      "Can be expensive for certain workloads",
      "Steeper learning curve for new users",
      "Some services have limited regional availability",
      "Documentation can be fragmented"
    ],
    "gettingStarted": [
      {
        "title": "Create an Azure account",
        "description": "Sign up for Azure and get started with free credits"
      },
      {
        "title": "Set up your environment",
        "description": "Configure your subscription, resource groups, and access controls"
      },
      {
        "title": "Choose compute services",
        "description": "Select from VMs, containers, or serverless based on your needs"
      },
      {
        "title": "Deploy resources",
        "description": "Launch your first GPU-enabled instance or AI service"
      }
    ],
    "computeServices": [
      {
        "name": "Azure Virtual Machines",
        "description": "GPU-enabled VMs for various workloads",
        "instanceTypes": [
          {
            "name": "NDm A100 v4",
            "description": "Powered by NVIDIA A100 Tensor Core GPUs for AI and HPC",
            "features": [
              "Up to 8 NVIDIA A100 80GB GPUs",
              "NVIDIA NVLink interconnect",
              "400 Gb/s NVIDIA Mellanox NDR InfiniBand"
            ]
          },
          {
            "name": "NC A100 v4",
            "description": "Optimized for deep learning and HPC",
            "features": [
              "NVIDIA A100 GPUs",
              "High-performance computing and AI training",
              "PCIe-based GPU connectivity"
            ]
          },
          {
            "name": "ND A100 v4",
            "description": "For large-scale AI training and inference",
            "features": [
              "NVIDIA A100 GPUs",
              "Optimized for distributed AI workloads",
              "High-bandwidth GPU interconnect"
            ]
          },
          {
            "name": "ND H100 v5",
            "description": "Flagship H100 SXM\u2011based training VMs for scale\u2011out GenAI.",
            "features": [
              "8\u00d7 NVIDIA H100 GPUs per VM",
              "3.2 Tbps interconnect per VM",
              "Dedicated 400 Gb/s CX7 per GPU"
            ]
          },
          {
            "name": "ND MI300X v5",
            "description": "AMD Instinct MI300X\u2011based VMs for AI training/inference.",
            "features": [
              "8\u00d7 AMD Instinct MI300X (192 GB each)",
              "4th\u2011Gen AMD Infinity Fabric links",
              "1850 GiB memory"
            ]
          },
          {
            "name": "NCads H100 v5",
            "description": "H100 NVL\u2011based VMs for training and batch inference.",
            "features": [
              "Up to 2\u00d7 NVIDIA H100 NVL (94 GB each)",
              "4th\u2011gen AMD EPYC Genoa CPUs",
              "Up to 640 GiB RAM"
            ]
          }
        ]
      },
      {
        "name": "Azure Kubernetes Service (AKS)",
        "description": "Managed Kubernetes service with GPU support",
        "features": [
          "Integrated GPU node pools",
          "Automated scaling and updates",
          "DevOps integration"
        ]
      },
      {
        "name": "Azure Machine Learning",
        "description": "End-to-end ML platform with GPU acceleration",
        "features": [
          "Automated ML capabilities",
          "Integrated MLOps",
          "Distributed training support"
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "GPU-Optimized Virtual Machines",
        "description": "Range of GPU-enabled VM sizes for different workloads",
        "types": [
          {
            "name": "NDm A100 v4",
            "gpuModel": "NVIDIA A100 80GB",
            "bestFor": "Large-scale AI training, HPC"
          },
          {
            "name": "NC A100 v4",
            "gpuModel": "NVIDIA A100",
            "bestFor": "Deep learning, HPC workloads"
          },
          {
            "name": "NV-series",
            "gpuModel": "NVIDIA Tesla M60",
            "bestFor": "Visualization, remote workstations"
          },
          {
            "name": "ND H100 v5",
            "gpuModel": "NVIDIA H100 SXM",
            "bestFor": "Scale-out training for GenAI and HPC"
          },
          {
            "name": "ND MI300X v5",
            "gpuModel": "AMD Instinct MI300X",
            "bestFor": "Training/inference with large VRAM per GPU"
          },
          {
            "name": "NCads H100 v5",
            "gpuModel": "NVIDIA H100 NVL",
            "bestFor": "High-throughput inference & training"
          }
        ]
      }
    ],
    "pricingOptions": [
      {
        "name": "Pay-as-you-go",
        "description": "Flexible pricing with no upfront commitment"
      },
      {
        "name": "Reserved VM Instances",
        "description": "Save up to 72% with 1 or 3-year commitments"
      },
      {
        "name": "Spot VMs",
        "description": "Up to 90% savings for interruptible workloads"
      },
      {
        "name": "Azure Hybrid Benefit",
        "description": "Cost savings for existing Windows Server and SQL Server licenses"
      }
    ],
    "regions": "60+ regions worldwide with multiple availability zones",
    "support": "Basic, Developer, Standard, and Professional Direct support plans with 24/7 options. Extensive documentation and community resources."
  },
  {
    "id": "8b627aa8-dfbb-40c2-9291-a23eab57b098",
    "name": "IBM Cloud",
    "description": "IBM Cloud provides NVIDIA GPU instances (including H200 and L40S) for training, fine\u2011tuning, and inference, with enterprise networking and support.",
    "pros": [
      "Strong focus on AI and data workloads",
      "Enterprise-grade support",
      "Integration with IBM's AI tools"
    ],
    "cons": [
      "Limited global presence compared to competitors",
      "Higher cost for some services",
      "Less community support"
    ],
    "link": "https://www.ibm.com/cloud/gpu",
    "slug": "ibm",
    "computeServices": [
      {
        "name": "IBM Virtual Servers for VPC",
        "description": "GPU\u2011accelerated virtual servers on IBM Cloud VPC.",
        "instanceTypes": [
          {
            "name": "H200",
            "description": "141 GB HBM3e for advanced AI workloads.",
            "features": [
              "Enterprise VPC networking",
              "GenAI tuned images",
              "Hourly pricing"
            ]
          },
          {
            "name": "L40S",
            "description": "48 GB for graphics and high\u2011throughput inference.",
            "features": [
              "Cost\u2011effective inference",
              "Supported frameworks"
            ]
          }
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "GPU Portfolio",
        "description": "Range of NVIDIA GPUs for AI workloads.",
        "types": [
          {
            "name": "H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Training & inference"
          },
          {
            "name": "L40S",
            "gpuModel": "NVIDIA L40S",
            "bestFor": "Inference & graphics"
          }
        ]
      }
    ]
  },
  {
    "id": "c53a1b15-f227-4ca5-be92-68e24a7643d9",
    "name": "Oracle Cloud",
    "description": "Oracle Cloud Infrastructure (OCI) offers bare\u2011metal and VM GPU shapes including H100, H200, L40S and A10, with Supercluster networking and large local NVMe.",
    "pros": [
      "Competitive pricing",
      "Strong performance for enterprise applications",
      "Good integration with Oracle products"
    ],
    "cons": [
      "Smaller ecosystem compared to AWS and GCP",
      "Limited documentation and community support",
      "Complexity in setup and management"
    ],
    "link": "https://www.oracle.com/cloud/compute/gpu.html",
    "slug": "oracle",
    "computeServices": [
      {
        "name": "OCI GPU Shapes",
        "description": "Bare\u2011metal and VM shapes for AI/HPC.",
        "instanceTypes": [
          {
            "name": "BM.GPU.H100",
            "description": "Bare\u2011metal H100 with up to 61.4 TB local NVMe per node.",
            "features": [
              "OCI Supercluster fabric",
              "Bare\u2011metal performance",
              "Checkpoint\u2011friendly storage"
            ]
          },
          {
            "name": "BM.GPU.H200",
            "description": "Bare\u2011metal H200 with HBM3e for large\u2011context workloads.",
            "features": [
              "RoCE/Supercluster networking",
              "High bandwidth"
            ]
          },
          {
            "name": "VM.GPU.L40S",
            "description": "VM shapes with NVIDIA L40S.",
            "features": [
              "Cost\u2011optimized inference",
              "Graphics/video"
            ]
          }
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "GPU Instances",
        "description": "NVIDIA and AMD GPU shapes (BM & VM).",
        "types": [
          {
            "name": "BM.GPU.H100",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Scale\u2011out training"
          },
          {
            "name": "BM.GPU.H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Training with larger memory/bandwidth"
          },
          {
            "name": "VM.GPU.L40S",
            "gpuModel": "NVIDIA L40S",
            "bestFor": "High\u2011throughput inference"
          }
        ]
      }
    ]
  },
  {
    "id": "1d434a66-bf40-40a8-8e80-d5ab48b6d27f",
    "name": "CoreWeave",
    "description": "CoreWeave is a specialized cloud provider offering GPU\u2011accelerated infrastructure (H100, H200 and more) for AI/ML, VFX, and batch compute, with Kubernetes\u2011native orchestration and rapid scale.",
    "pros": [
      "Extensive selection of NVIDIA GPUs, including latest models",
      "Up to 35x faster and 80% less expensive than legacy cloud providers",
      "Kubernetes-native infrastructure for easy scaling and deployment",
      "Rapid deployment with ability to access thousands of GPUs in seconds",
      "Specialized support for AI, machine learning, and rendering workloads",
      "NVIDIA Elite Cloud Solutions Provider for both Compute and Visualization",
      "Fully-managed, bare metal serverless Kubernetes infrastructure"
    ],
    "cons": [
      "Primary focus on North American data centers",
      "Specialized nature may not suit all general computing needs",
      "Newer player compared to established cloud giants",
      "Learning curve for users unfamiliar with Kubernetes"
    ],
    "link": "https://www.coreweave.com/",
    "slug": "coreweave",
    "computeServices": [
      {
        "name": "GPU Instances",
        "description": "NVIDIA HGX H100/H200 nodes and other SKUs at supercomputer scale.",
        "instanceTypes": [
          {
            "name": "HGX H100",
            "description": "8\u00d7 H100 SXM nodes for training and inference.",
            "features": [
              "Available at supercomputer scale",
              "Quantum\u20112 InfiniBand fabric",
              "Pricing from ~$2.23/hr per GPU (varies)"
            ]
          },
          {
            "name": "HGX H200",
            "description": "8\u00d7 H200 SXM nodes with HBM3e for fast LLM training/serving.",
            "features": [
              "HBM3e memory (141 GB/GPU)",
              "Designed for GenAI and HPC",
              "Reserve capacity or on\u2011demand where available"
            ]
          }
        ]
      }
    ]
  },
  {
    "id": "4a4fdeae-7d4f-4d75-9967-54bbd498e4bf",
    "name": "Vast.ai",
    "description": "Vast.ai is a GPU marketplace with live, transparent pricing (including H200/H100) across thousands of community and datacenter hosts.",
    "pros": [
      "Cost-effective (5-6X cheaper than traditional cloud services)",
      "Flexible pricing with on-demand and interruptible options",
      "Real-time bidding system for cost optimization",
      "Docker ecosystem for quick software deployment",
      "Enterprise-grade security and compliance",
      "Supports various AI and deep learning workloads",
      "CLI support for automated deployment"
    ],
    "cons": [
      "Primarily focused on Linux-based Docker instances",
      "No Windows support",
      "Limited GUI options (SSH, Jupyter, or command-only)",
      "No remote desktop functionality",
      "Performance may vary across different providers"
    ],
    "link": "https://vast.ai/",
    "slug": "vast",
    "computeServices": [
      {
        "name": "Marketplace Instances",
        "description": "On\u2011demand GPU rentals with live bidding and filters.",
        "instanceTypes": [
          {
            "name": "H200",
            "description": "HBM3e 141 GB with high bandwidth.",
            "features": [
              "Live pricing & reputation scores",
              "Wide region coverage"
            ]
          },
          {
            "name": "H100 SXM",
            "description": "80 GB SXM5 nodes via verified providers.",
            "features": [
              "NVLink/NVSwitch",
              "Multi\u2011GPU nodes"
            ]
          }
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "Available GPUs",
        "description": "Catalog of supported GPUs and filters.",
        "types": [
          {
            "name": "H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Training & inference at high BW"
          },
          {
            "name": "H100 SXM",
            "gpuModel": "NVIDIA H100 SXM5",
            "bestFor": "Frontier\u2011class training"
          }
        ]
      }
    ]
  },
  {
    "id": "825cef3b-54f5-426e-aa29-c05fe3070833",
    "name": "Lambda Labs",
    "description": "Lambda is an AI compute platform offering on\u2011demand NVIDIA GPUs, including H100 and Blackwell B200, plus private clusters with Quantum\u20112 InfiniBand.",
    "pros": [
      "Early access to latest NVIDIA GPUs (H100, H200, Blackwell)",
      "Specialized for AI workloads",
      "One-click Jupyter access",
      "Pre-installed popular ML frameworks",
      "Multi-GPU instances available (up to 8x GPUs)",
      "Developer-friendly API",
      "Offers both on-demand and reserved GPU clusters",
      "High-speed NVIDIA Quantum-2 InfiniBand networking"
    ],
    "cons": [
      "Primarily focused on AI and ML workloads",
      "Limited global data center presence compared to major cloud providers",
      "Newer player in the cloud GPU market",
      "May have higher costs for non-AI workloads"
    ],
    "link": "https://lambdalabs.com/",
    "slug": "lambda"
  },
  {
    "id": "a4c4b4ea-4de7-4e04-8d40-d4c4fc1d8182",
    "name": "Fluidstack",
    "description": "Fluidstack provides on\u2011demand GPUs from H200/H100 to A100 and L40S, plus private clusters on request.",
    "pros": [
      "Highly cost-effective (30-80% lower costs compared to major cloud providers)",
      "Large-scale GPU availability (10,000+ NVIDIA H100 GPUs deployed)",
      "Rapid deployment and scaling capabilities",
      "Fully managed infrastructure with 24/7 support",
      "Flexible options from on-demand instances to reserved clusters",
      "MLOps services included at no extra cost",
      "Wide range of GPU models available",
      "Enterprise-grade security and compliance"
    ],
    "cons": [
      "Relatively newer and smaller compared to major cloud providers",
      "Primary focus on AI and ML workloads may not suit all use cases",
      "Limited global presence compared to hyperscalers",
      "Less established brand recognition in the broader cloud market"
    ],
    "link": "https://www.fluidstack.io/",
    "slug": "fluidstack",
    "computeServices": [
      {
        "name": "GPU Instances",
        "description": "On\u2011demand dedicated GPUs for AI workloads.",
        "instanceTypes": [
          {
            "name": "H200 SXM",
            "description": "141 GB HBM3e.",
            "features": [
              "From around $2.30/GPU\u2011hr (public list)",
              "Regions by request"
            ]
          },
          {
            "name": "H100 SXM",
            "description": "80 GB HBM.",
            "features": [
              "From around $2.10/GPU\u2011hr (public list)",
              "Cluster options"
            ]
          }
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "GPU Lineup",
        "description": "Range of NVIDIA GPUs.",
        "types": [
          {
            "name": "H200 SXM",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Training/inference"
          },
          {
            "name": "H100 SXM",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training/inference"
          },
          {
            "name": "A100 80GB",
            "gpuModel": "NVIDIA A100",
            "bestFor": "Training/economical"
          },
          {
            "name": "L40S",
            "gpuModel": "NVIDIA L40S",
            "bestFor": "Inference & media"
          }
        ]
      }
    ]
  },
  {
    "id": "8b1f3feb-2c2f-4983-a451-2564dccc8917",
    "name": "Genesis Cloud",
    "slug": "genesis",
    "description": "Genesis Cloud offers HGX H100/H200 and B200 clusters with 3.2 Tbps InfiniBand, plus RTX options for cost\u2011sensitive workloads.",
    "link": "https://www.genesiscloud.com/",
    "docsLink": "https://docs.genesiscloud.com/",
    "features": [
      {
        "title": "Enterprise AI Cloud",
        "description": "End-to-end machine learning platforms with high reliability and scalability"
      },
      {
        "title": "High Performance",
        "description": "35x more performance for LLMs, GenAI, and large multi-node trainings"
      },
      {
        "title": "EU Sovereign Cloud",
        "description": "AI workloads under EU regulations with enhanced security"
      },
      {
        "title": "Green Infrastructure",
        "description": "100% green energy powered data centers with low PUE"
      }
    ],
    "pros": [
      "80% less expensive compared to legacy cloud providers",
      "35x more performance for LLMs and GenAI workloads",
      "EU sovereign cloud compliance",
      "100% green energy infrastructure",
      "ISO27001 certified data centers",
      "Up to 100 Gbps internet connectivity",
      "99.9% guaranteed uptime",
      "Early access to latest NVIDIA GPUs (B200, GB200)"
    ],
    "cons": [
      "Limited global presence (primarily EU-focused)",
      "Newer player compared to major cloud providers",
      "Specialized focus may not suit all computing needs"
    ],
    "computeServices": [
      {
        "name": "NVIDIA GPU Instances",
        "description": "Various NVIDIA GPU options for AI and ML workloads",
        "instanceTypes": [
          {
            "name": "NVIDIA HGX H100",
            "description": "Superior AI training and inference workloads with SXM5 H100",
            "features": [
              "MLPerf benchmark-dominating GPU",
              "30x AI inference over Ampere generation",
              "3.2 Tbps InfiniBand connectivity",
              "Starting from $2.00/h per GPU"
            ]
          },
          {
            "name": "NVIDIA Blackwell Architecture",
            "description": "Next-generation AI superchip (Coming Soon)",
            "features": [
              "Up to 4x faster AI training than previous generation",
              "Up to 30x speed up in real-time LLM inference",
              "Secure AI protecting LLMs and sensitive data"
            ]
          }
        ]
      },
      {
        "name": "GPU Nodes",
        "description": "HGX nodes for AI training, plus single\u2011GPU options.",
        "instanceTypes": [
          {
            "name": "HGX H100",
            "description": "8\u00d7 H100 SXM5 per node.",
            "features": [
              "3.2 Tbps InfiniBand",
              "2 TB DDR5 system RAM",
              "Multi\u2011NVMe"
            ]
          },
          {
            "name": "HGX H200",
            "description": "8\u00d7 H200 SXM per node.",
            "features": [
              "HBM3e 141 GB",
              "High bandwidth",
              "Node\u2011level NVMe"
            ]
          },
          {
            "name": "HGX B200",
            "description": "8\u00d7 B200 per node.",
            "features": "[180 GB HBM3e,Request/limited availability]"
          }
        ]
      }
    ],
    "support": "Knowledge base, developer documentation, and dedicated support team",
    "regions": "Tier 3 data centers in the European Union",
    "uniqueSellingPoints": [
      "Built on NVIDIA's reference architecture",
      "Optimized for AI and ML workloads",
      "No ingress or egress fees",
      "High-bandwidth network optimized for multi-node operations"
    ],
    "gpuServices": [
      {
        "name": "GPU Options",
        "description": "Portfolio of NVIDIA GPUs.",
        "types": [
          {
            "name": "HGX H100",
            "gpuModel": "NVIDIA H100 SXM5",
            "bestFor": "Training at scale"
          },
          {
            "name": "HGX H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Training/serving with larger memory"
          },
          {
            "name": "HGX B200",
            "gpuModel": "NVIDIA B200",
            "bestFor": "Next\u2011gen performance"
          }
        ]
      }
    ]
  },
  {
    "id": "fd8bfdf8-162d-4a95-954d-ca4279edc46f",
    "name": "Datacrunch",
    "slug": "datacrunch",
    "description": "DataCrunch provides premium GPU instances with fixed and dynamic pricing, including H100 SXM5 on\u2011demand and H200 nodes.",
    "pros": [
      "Wide range of GPU models, including latest NVIDIA H200",
      "Cost-effective compared to major cloud providers",
      "Streamlined and user-friendly interface",
      "Excellent documentation and API",
      "Flexible pricing with pay-as-you-go model",
      "Based in EU, potentially easing GDPR compliance",
      "Renewable energy-powered data centers"
    ],
    "cons": [
      "Limited global presence (primarily Europe-focused)",
      "Smaller company compared to major cloud providers",
      "Specialized focus may not suit all computing needs",
      "Fewer data center locations than larger competitors"
    ],
    "link": "https://datacrunch.io/",
    "computeServices": [
      {
        "name": "Premium GPU Instances",
        "description": "Dedicated GPU VMs with transparent pricing.",
        "instanceTypes": [
          {
            "name": "H100 SXM5",
            "description": "80 GB HBM; dynamic or fixed pricing.",
            "features": [
              "On\u2011demand launch",
              "3200 Gbps RDMA fabrics"
            ]
          },
          {
            "name": "H200",
            "description": "8\u00d7 H200 nodes available in select regions.",
            "features": [
              "HBM3e 141 GB",
              "Reserved and on\u2011demand options"
            ]
          }
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "GPU Catalog",
        "description": "Range of NVIDIA GPUs and pricing modes.",
        "types": [
          {
            "name": "H100 SXM5",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training & inference"
          },
          {
            "name": "H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Larger context & bandwidth"
          }
        ]
      }
    ]
  },
  {
    "id": "54cc0c05-b0e6-49b3-95fb-831b36dd7efd",
    "name": "Hyperstack",
    "slug": "hyperstack",
    "description": "Hyperstack offers on\u2011demand H200/H100/A100/L40 with clear per\u2011GPU hourly pricing and reservation discounts.",
    "link": "https://www.hyperstack.cloud/",
    "docsLink": "https://www.hyperstack.cloud/documentation",
    "features": [
      {
        "title": "Managed Kubernetes",
        "description": "Automated software deployment, scaling and management"
      },
      {
        "title": "Pre-Configured Flavours",
        "description": "Optimized templates for GPU-accelerated workloads with custom flavor options"
      },
      {
        "title": "First Class API",
        "description": "Purpose-built API designed specifically for GPU cloud operations"
      },
      {
        "title": "Optimised Networking",
        "description": "Network architecture optimized for maximum GPU efficiency"
      },
      {
        "title": "Premium Storage",
        "description": "Multiple storage options including NVMe, HDD block, and HDD Shared storage"
      }
    ],
    "pros": [
      "Up to 75% more cost-effective than major cloud providers",
      "GPU-optimized ecosystem for maximum performance efficiency",
      "Easy-to-use platform with 1-click deployments",
      "100% renewable energy powered infrastructure",
      "NVIDIA Preferred NCP Partner",
      "Enterprise-grade features and support",
      "Role-based access control"
    ],
    "cons": [
      "Limited global presence (primarily Europe and North America)",
      "Niche market focus on GPU workloads",
      "Less established compared to major cloud providers"
    ],
    "gettingStarted": [
      {
        "title": "Register Account",
        "description": "Sign up for Hyperstack cloud services"
      },
      {
        "title": "Choose Configuration",
        "description": "Select from pre-configured flavors or create custom ones"
      },
      {
        "title": "Deploy Resources",
        "description": "Launch GPU instances with 1-click deployment"
      }
    ],
    "computeServices": [
      {
        "name": "GPU Instances",
        "description": "Range of NVIDIA GPU options for various workloads",
        "instanceTypes": [
          {
            "name": "NVIDIA H100 SXM",
            "description": "High-performance GPU instances for AI and ML workloads",
            "features": [
              "On-demand pricing from $3.00/hour",
              "Reserved pricing from $2.10/hour",
              "Scalable from 8 to 16,384 GPUs"
            ]
          }
        ]
      },
      {
        "name": "On\u2011Demand GPU",
        "description": "Configure per\u2011GPU CPU/RAM with transparent pricing.",
        "instanceTypes": [
          {
            "name": "H200 SXM",
            "description": "141 GB HBM3e.",
            "features": [
              "~$3.50/GPU\u2011hr public list",
              "Reservation discounts"
            ]
          },
          {
            "name": "H100 (PCIe & NVLink)",
            "description": "80 GB HBM.",
            "features": [
              "From ~$1.90\u2013$1.95/GPU\u2011hr",
              "NVLink option"
            ]
          },
          {
            "name": "A100 (SXM/NVLink)",
            "description": "80 GB HBM.",
            "features": [
              "From ~$1.35\u2013$1.60/GPU\u2011hr",
              "NVLink option"
            ]
          }
        ]
      }
    ],
    "regions": "Data centers in Europe and North America",
    "support": "Human support team, documentation, and developer resources",
    "uniqueSellingPoints": [
      "100% renewably powered by hydro-energy",
      "20x more energy-efficient than traditional computing",
      "99.982% uptime guarantee",
      "Free air cooling in data centers",
      "Multi-region support"
    ],
    "gpuServices": [
      {
        "name": "GPU Models",
        "description": "NVIDIA lineup for AI workloads.",
        "types": [
          {
            "name": "H200 SXM",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Large models; high BW"
          },
          {
            "name": "H100",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training & inference"
          },
          {
            "name": "A100 80GB",
            "gpuModel": "NVIDIA A100",
            "bestFor": "Balanced"
          },
          {
            "name": "L40",
            "gpuModel": "NVIDIA L40",
            "bestFor": "Vision/media"
          }
        ]
      }
    ]
  },
  {
    "id": "6f6f477e-c195-4403-a07a-4cf9faa65a08",
    "name": "The Cloud Minders",
    "slug": "cloud-minders",
    "description": "The Cloud Minders (now merging into QumulusAI) provides dedicated H100/H200 clusters; site transition announced for Oct 31, 2025.",
    "link": "https://www.thecloudminders.com",
    "docsLink": null,
    "features": [
      {
        "title": "Purpose-Built AI Clouds",
        "description": "Custom solutions optimized for specific AI/ML workload needs."
      },
      {
        "title": "Bleeding Edge GPUs",
        "description": "Equipped with the latest NVIDIA GPUs, including H100 and H200."
      },
      {
        "title": "Industry Leading CPUs",
        "description": "EPYC CPUs with clock speeds over 3.0 GHz for faster processing."
      },
      {
        "title": "AI-Optimized Platform",
        "description": "Integrates smoothly with popular AI/ML frameworks and tools."
      },
      {
        "title": "NVMe Storage",
        "description": "Ultra-fast NVMe storage for handling large datasets and numerous small files."
      },
      {
        "title": "Flexible Options",
        "description": "Supports Docker containers, VMs, and bare metal servers optimized and accelerated by the latest GPUs."
      }
    ],
    "pros": [
      "Access to the latest NVIDIA GPUs like H200.",
      "Purpose-built infrastructure optimized for AI/ML workloads.",
      "Flexible deployment options (VMs, Docker, Bare Metal).",
      "Transparent pricing model with no hidden fees.",
      "Potential for workload benchmarking on H200 vs. H100.",
      "High-speed networking and super-fast storage.",
      "Strong focus on customer partnership and support."
    ],
    "cons": [
      "Specific geographic availability not explicitly stated.",
      "May be less established compared to major hyperscalers.",
      "Details on specific data center certifications (beyond SOC 1 Type 2) are not provided."
    ],
    "gettingStarted": [
      {
        "title": "Contact TCM",
        "description": "Reach out to The Cloud Minders to discuss your specific AI infrastructure needs."
      },
      {
        "title": "Benchmark Workload (Optional)",
        "description": "Sign up to benchmark your workload on an 8x H200 server."
      },
      {
        "title": "Choose Deployment Option",
        "description": "Select from VM images, Docker containers, or bare metal GPUs based on your requirements."
      }
    ],
    "computeServices": [
      {
        "name": "GPU Instances",
        "description": "Variety of NVIDIA GPU options for different AI/ML tasks.",
        "instanceTypes": [
          {
            "name": "Nvidia H200 SXM",
            "description": "High-performance GPU for large AI models.",
            "features": [
              "141GB vRAM",
              "4.8TB/s memory bandwidth",
              "On-demand pricing starting at $4.85/GPU/Hr"
            ]
          },
          {
            "name": "Nvidia H100 SXM",
            "description": "High-performance GPU for advanced AI and vision tasks",
            "features": [
              "80GB vRAM",
              "On-demand pricing starting at $4.52/Hr"
            ]
          },
          {
            "name": "Nvidia H100 NVL",
            "description": "Optimized for high-throughput inference and complex NLP tasks.",
            "features": [
              "94GB vRAM",
              "On-demand pricing starting at $4.05/Hr"
            ]
          },
          {
            "name": "RTX A5000",
            "description": "Suitable for object detection, creative AI tasks, and text-to-image generation.",
            "features": [
              "24GB vRAM",
              "On-demand pricing starting at $0.55/Hr"
            ]
          },
          {
            "name": "RTX 4000 Ada",
            "description": "Good for Image segmentation, facial recognition, medical imaging",
            "features": [
              "20GB vRAM",
              "On-demand pricing starting at $0.55/Hr"
            ]
          },
          {
            "name": "RTX A4000",
            "description": "Compact inference, real-time audio processing, mobile AI",
            "features": [
              "16GB vRAM",
              "On-demand pricing starting at $0.40/Hr"
            ]
          },
          {
            "name": "V100",
            "description": "Image classification, sequential data analysis, NLP fine-tuning",
            "features": [
              "16GB vRAM",
              "On-demand pricing starting at $0.24/Hr"
            ]
          }
        ]
      },
      {
        "name": "Dedicated GPU Clusters",
        "description": "Tailored AI infrastructure on latest NVIDIA GPUs.",
        "instanceTypes": [
          {
            "name": "H200 Clusters",
            "description": "8\u00d7 H200 nodes for training and inference.",
            "features": [
              "Priority access",
              "Custom networking"
            ]
          },
          {
            "name": "H100 Clusters",
            "description": "8\u00d7 H100 nodes.",
            "features": [
              "Custom topologies",
              "Bare\u2011metal options"
            ]
          }
        ]
      }
    ],
    "regions": "Data center with SOC 1 Type 2 certification, high-speed connectivity, fault-tolerant storage, and round-the-clock security.",
    "support": "Remote support available. Partnership approach with direct team interaction.",
    "uniqueSellingPoints": [
      "Supercompute as a Service tailored for AI training and inference.",
      "Access to the latest NVIDIA H200 GPUs.",
      "Transparent pricing with no hidden fees.",
      "Option to benchmark workloads on cutting-edge hardware.",
      "Best-in-class data center infrastructure with reliability and security."
    ],
    "gpuServices": [
      {
        "name": "GPU Options",
        "description": "High\u2011end NVIDIA accelerators.",
        "types": [
          {
            "name": "H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Large training runs"
          },
          {
            "name": "H100",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training/inference"
          }
        ]
      }
    ]
  },
  {
    "id": "c58cd5f6-4bbc-454a-abbf-fad2b94180c6",
    "name": "Paperspace",
    "slug": "paperspace",
    "description": "Paperspace (by DigitalOcean) offers dedicated GPUs including H100/HGX H100 and A100 with managed notebooks and workflows.",
    "link": "https://www.paperspace.com/",
    "docsLink": "https://docs.digitalocean.com/products/paperspace/",
    "features": [
      {
        "title": "Gradient Notebooks",
        "description": "Interactive Jupyter notebooks with a free tier, pre-configured templates, and access to powerful GPUs."
      },
      {
        "title": "Gradient Deployments",
        "description": "Serve machine learning models as scalable API endpoints."
      },
      {
        "title": "Core Machines",
        "description": "High-performance virtual machines with a wide variety of NVIDIA GPUs for demanding workloads."
      },
      {
        "title": "Team Collaboration",
        "description": "Features for teams to collaborate on projects, including shared drives and SSO."
      },
      {
        "title": "Persistent Storage",
        "description": "Offers persistent storage that can be shared across different machines and notebooks."
      },
      {
        "title": "API and CLI",
        "description": "Programmatic access to the Paperspace platform to automate workflows."
      }
    ],
    "pros": [
      "User-friendly interface and easy to get started",
      "Wide range of NVIDIA GPU options",
      "Free GPU and CPU plans for notebooks",
      "Pay-per-second billing model",
      "Comprehensive MLOps platform (Gradient)",
      "Strong community forum and extensive documentation"
    ],
    "cons": [
      "Limited data center regions (US and Europe)",
      "Some user reports of inconsistent customer support",
      "Availability of high-end multi-GPU instances can be limited",
      "The platform is undergoing changes after being acquired by DigitalOcean"
    ],
    "gettingStarted": [
      {
        "title": "Create an Account",
        "description": "Sign up for a Paperspace account. You can start with a free plan."
      },
      {
        "title": "Choose a Product",
        "description": "Select the product that best fits your needs, such as Gradient Notebooks for interactive development or Core Machines for more control."
      },
      {
        "title": "Select a Template and Machine",
        "description": "Choose a pre-configured template for your desired framework and select a GPU or CPU instance."
      },
      {
        "title": "Start Working",
        "description": "Launch your machine or notebook and begin building, training, or deploying your AI applications."
      }
    ],
    "computeServices": [
      {
        "name": "Core GPU Instances",
        "description": "Virtual machines with a wide range of NVIDIA GPUs for various workloads.",
        "instanceTypes": [
          {
            "name": "NVIDIA H100",
            "description": "Top-tier GPU for large-scale AI model training and inference.",
            "features": [
              "80GB of VRAM",
              "PCIe interface",
              "Available in single and multi-GPU configurations"
            ]
          },
          {
            "name": "NVIDIA A100",
            "description": "Powerful GPU for AI, data analytics, and HPC.",
            "features": [
              "40GB and 80GB VRAM options",
              "SXM4 and PCIe interconnects",
              "Available in multi-GPU configurations"
            ]
          },
          {
            "name": "NVIDIA A6000",
            "description": "Professional GPU for graphics-intensive workloads, rendering, and AI.",
            "features": [
              "48GB of VRAM",
              "Suitable for professional visualization and AI",
              "Available in multi-GPU configurations"
            ]
          },
          {
            "name": "NVIDIA A5000 / A4000",
            "description": "Cost-effective GPUs for a variety of AI and graphics workloads.",
            "features": [
              "24GB (A5000) and 16GB (A4000) of VRAM",
              "Balanced performance for development and smaller-scale training"
            ]
          },
          {
            "name": "NVIDIA RTX 5000 / RTX 4000",
            "description": "Quadro GPUs suitable for professional graphics and some ML tasks.",
            "features": [
              "16GB (RTX 5000) and 8GB (RTX 4000) of VRAM",
              "Real-time ray tracing capabilities"
            ]
          }
        ]
      },
      {
        "name": "Gradient Notebooks",
        "description": "Managed Jupyter notebooks for interactive development and experimentation.",
        "features": [
          "Free CPU and GPU tiers",
          "1-click launch with pre-configured templates",
          "Persistent storage for your projects",
          "Collaboration features for teams"
        ]
      },
      {
        "name": "Gradient Deployments",
        "description": "A serverless product for deploying trained models as REST APIs.",
        "features": [
          "Scalable and managed infrastructure",
          "Integration with Gradient Notebooks and Workflows",
          "Real-time monitoring of deployed models"
        ]
      },
      {
        "name": "Dedicated GPUs",
        "description": "On\u2011demand GPU VMs and notebooks.",
        "instanceTypes": [
          {
            "name": "HGX H100",
            "description": "80 GB HBM; multi\u2011GPU configurations up to 8x.",
            "features": [
              "Docs list from ~$2.24/GPU\u2011hr depending on plan",
              "Managed workflows"
            ]
          },
          {
            "name": "H100 (PCIe)",
            "description": "80 GB HBM; single\u2011GPU option.",
            "features": [
              "Docs show from ~$3.09/GPU\u2011hr",
              "Pause/resume to save idle cost"
            ]
          },
          {
            "name": "A100 80GB",
            "description": "80 GB HBM.",
            "features": [
              "Multiple regions",
              "Gradient integrations"
            ]
          }
        ]
      }
    ],
    "regions": "Data centers are located in the US (Secaucus, NJ and Santa Clara, CA) and Europe (Amsterdam, NL).",
    "support": "Support is available through a community forum, extensive documentation, and a support ticket system.",
    "uniqueSellingPoints": [
      "Simplicity and developer-focused user experience",
      "A comprehensive MLOps platform (Gradient) for the entire ML lifecycle",
      "Offers a free tier with GPU access for notebooks",
      "Strong community and a showcase of public projects",
      "Now part of the DigitalOcean ecosystem"
    ],
    "gpuServices": [
      {
        "name": "GPU Types",
        "description": "Available NVIDIA accelerators.",
        "types": [
          {
            "name": "HGX H100",
            "gpuModel": "NVIDIA H100 SXM5",
            "bestFor": "Training at scale"
          },
          {
            "name": "H100 PCIe",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training/inference"
          },
          {
            "name": "A100 80GB",
            "gpuModel": "NVIDIA A100",
            "bestFor": "Balanced workloads"
          }
        ]
      }
    ]
  },
  {
    "id": "ee80fbec-5f7f-4f44-b6bb-d70042b0a799",
    "name": "TensorWave",
    "slug": "tensorwave",
    "description": "TensorWave is an AMD\u2011first AI cloud with bare\u2011metal and managed clusters built on Instinct MI300X/MI325X/MI355X accelerators with large VRAM and ROCm.",
    "link": "https://tensorwave.com/",
    "docsLink": "https://docs.tensorwave.com/",
    "features": [
      {
        "title": "AMD Instinct Accelerators",
        "description": "Powered by AMD Instinct\u2122 Series GPUs for high-performance AI workloads."
      },
      {
        "title": "High VRAM GPUs",
        "description": "Offers instances with 192GB of VRAM per GPU, ideal for large models."
      },
      {
        "title": "Bare Metal & Kubernetes",
        "description": "Provides both bare metal servers for maximum control and managed Kubernetes for orchestration."
      },
      {
        "title": "Direct Liquid Cooling",
        "description": "Utilizes direct liquid cooling to reduce data center energy costs and improve efficiency."
      },
      {
        "title": "High-Speed Network Storage",
        "description": "Features high-speed network storage to support demanding AI pipelines."
      },
      {
        "title": "ROCm Software Ecosystem",
        "description": "Leverages the AMD ROCm open software ecosystem to avoid vendor lock-in."
      }
    ],
    "pros": [
      "Specialized in high-performance AMD GPUs",
      "Offers GPUs with large VRAM (192GB)",
      "Claims better price-to-performance than competitors",
      "Provides 'white-glove' onboarding and support",
      "Utilizes an open-source software stack (ROCm)",
      "Offers bare metal access for greater control"
    ],
    "cons": [
      "A newer and less established company (founded in 2023)",
      "Exclusively focused on AMD, which may be a limitation for some users",
      "Limited publicly available information on pricing",
      "A smaller ecosystem when compared to major cloud providers"
    ],
    "gettingStarted": [
      {
        "title": "Request Access",
        "description": "Sign up on the TensorWave website to get access to their platform."
      },
      {
        "title": "Choose a Service",
        "description": "Select between Bare Metal servers or a managed Kubernetes cluster."
      },
      {
        "title": "Follow Quickstarts",
        "description": "Utilize the documentation and quick-start guides for PyTorch, Docker, Kubernetes, and other tools."
      },
      {
        "title": "Deploy Your Model",
        "description": "Deploy your AI model for training, fine-tuning, or inference."
      }
    ],
    "computeServices": [
      {
        "name": "AMD GPU Instances",
        "description": "Bare metal servers and managed Kubernetes clusters with AMD Instinct GPUs.",
        "instanceTypes": [
          {
            "name": "AMD Instinct MI300X",
            "description": "High-performance GPU with 192GB of HBM3 memory, suitable for large language models and generative AI.",
            "features": [
              "192GB of HBM3 memory",
              "Ideal for large-scale AI model training and inference",
              "High-speed interconnects"
            ]
          },
          {
            "name": "AMD Instinct MI325X",
            "description": "Next\u2011generation accelerator with expanded memory architecture for AI/HPC.",
            "features": [
              "256 GB HBM3 per GPU",
              "Optimized for large\u2011scale model training",
              "Improved bandwidth over MI300X"
            ]
          },
          {
            "name": "AMD Instinct MI355X",
            "description": "2025 flagship with 288 GB HBM3e and ~8 TB/s bandwidth.",
            "features": [
              "288 GB HBM3e per GPU",
              "Designed for very large context windows and multimodal",
              "Offered as dedicated bare metal"
            ]
          }
        ]
      },
      {
        "name": "Managed Kubernetes",
        "description": "Kubernetes clusters for orchestrated AI workloads.",
        "features": [
          "Scalable from 8 to 1024 GPUs",
          "Interconnected with 3.2TB/s RoCE v2 networking"
        ]
      },
      {
        "name": "Inference Platform (Manifest)",
        "description": "An enterprise inference platform designed for larger context windows and reduced latency.",
        "features": [
          "Accelerated reasoning",
          "Secure and private data storage"
        ]
      }
    ],
    "regions": "Primary data center and headquarters are located in Las Vegas, Nevada. The company is building the largest AMD-specific AI training cluster in North America.",
    "support": "Offers 'white-glove' onboarding and support, extensive documentation, and a company blog.",
    "uniqueSellingPoints": [
      "Exclusive focus on AMD Instinct GPUs",
      "First-to-market with the latest AMD Instinct models",
      "Building the largest AMD-specific AI training cluster in North America",
      "Emphasis on an open-source software stack (ROCm) to prevent vendor lock-in",
      "High-memory GPUs (192GB) as a standard offering"
    ]
  },
  {
    "id": "c45c33ad-24c9-4ad7-ac37-ca0e30e63434",
    "name": "Crusoe",
    "slug": "crusoe",
    "description": "Crusoe Cloud offers sustainable, high\u2011performance GPU compute (A100, L40S, H200 and emerging Blackwell) powered by stranded energy and Tier III facilities.",
    "link": "https://www.crusoe.com/",
    "docsLink": "https://docs.crusoecloud.com/",
    "features": [
      {
        "title": "Sustainable Computing",
        "description": "Powered by stranded and wasted energy sources to reduce carbon emissions"
      },
      {
        "title": "Fast Spin Up",
        "description": "Machines are deployed quickly when they are launched"
      },
      {
        "title": "Persistent Storage",
        "description": "Disk storage that can be used across multiple instances"
      },
      {
        "title": "SXM Support",
        "description": "Provides instances with NVIDIA SXM GPU interconnects"
      },
      {
        "title": "SLA Guarantee",
        "description": "Provider offers an SLA on uptime and performance"
      }
    ],
    "pros": [
      "Great GPU availability for on-demand use",
      "Affordable pricing for GPU instances",
      "Environmentally sustainable approach using stranded energy",
      "SLA guarantee on uptime and performance",
      "Lowest market price for A100_80G GPUs",
      "Fast instance deployment"
    ],
    "cons": [
      "Limited GPU variety (primarily A100, A100_80G, and L40S)",
      "Regional focus rather than global presence",
      "Newer player in the cloud GPU market",
      "Limited GPU interconnect options compared to some competitors"
    ],
    "gettingStarted": [
      {
        "title": "Create an account",
        "description": "Sign up for Crusoe Cloud services through their website"
      },
      {
        "title": "Select GPU instance",
        "description": "Choose from available GPU types (A100, A100_80G, or L40S)"
      },
      {
        "title": "Configure storage",
        "description": "Set up persistent storage options for your workloads"
      },
      {
        "title": "Launch instance",
        "description": "Deploy your instance with fast spin-up times"
      }
    ],
    "computeServices": [
      {
        "name": "GPU Instances",
        "description": "High-performance GPU instances powered by NVIDIA GPUs",
        "instanceTypes": [
          {
            "name": "A100",
            "description": "40GB NVIDIA A100 GPU instances for AI and ML workloads",
            "features": [
              "Limited availability",
              "Cost-effective pricing",
              "Suitable for deep learning and HPC workloads"
            ]
          },
          {
            "name": "A100_80G",
            "description": "80GB NVIDIA A100 GPU instances for memory-intensive AI workloads",
            "features": [
              "High availability",
              "Lowest market price",
              "Ideal for large model training and inference"
            ]
          },
          {
            "name": "L40S",
            "description": "NVIDIA L40S GPU instances for AI and graphics workloads",
            "features": [
              "High availability",
              "Competitive pricing",
              "Good balance of performance and cost"
            ]
          },
          {
            "name": "H200",
            "description": "NVIDIA H200 141GB SXM5 instances for cutting\u2011edge AI workloads.",
            "features": [
              "On\u2011demand availability in select regions",
              "HBM3e for higher memory bandwidth",
              "Suited for LLM training/serving"
            ]
          },
          {
            "name": "B200",
            "description": "Blackwell B200 instances (availability by request).",
            "features": [
              "Next\u2011gen performance for training/inference",
              "Contact sales for capacity",
              "Quantum\u20112 networking on clusters"
            ]
          }
        ]
      },
      {
        "name": "Reserved Clusters",
        "description": "Large-scale reserved GPU clusters for enterprise and research workloads",
        "features": [
          "Custom node configurations",
          "Flexible interconnect options",
          "Long-term reservation for consistent availability"
        ]
      }
    ],
    "regions": "Strategically located data centers in low-cost energy areas of the United States",
    "support": "Documentation, SOC II certified infrastructure",
    "uniqueSellingPoints": [
      "Environmentally sustainable cloud computing using stranded energy sources",
      "Lowest market price for A100_80G GPUs",
      "Founded in 2018 with $350M Series C funding",
      "Integration with Shadeform for console and API access",
      "Focuses on balancing performance, availability, and environmental impact"
    ]
  },
  {
    "id": "b046d087-96e5-4bd0-9c96-060164cf9a04",
    "name": "Vultr",
    "slug": "vultr",
    "description": "Vultr offers global access to the latest AMD and NVIDIA GPUs for AI/ML, AR/VR, high-performance computing, VDI/CAD, and more, available on demand either as virtual machines or bare metal across 32 worldwide data center regions.",
    "link": "https://www.vultr.com/products/cloud-gpu/",
    "docsLink": "https://www.vultr.com/docs/",
    "features": [
      {
        "title": "AMD and NVIDIA GPU Options",
        "description": "Access to diverse GPU options including AMD Instinct and NVIDIA Tensor Core GPUs"
      },
      {
        "title": "Global Availability",
        "description": "Deploy GPU resources across 32 cloud data center regions worldwide"
      },
      {
        "title": "Kubernetes Support",
        "description": "Vultr Kubernetes Engine for GPU-accelerated containerized workloads"
      },
      {
        "title": "Serverless Inference",
        "description": "Deploy and scale GenAI models quickly with Vultr Serverless Inference"
      },
      {
        "title": "Virtual Machines and Bare Metal",
        "description": "Choose between GPU-accelerated VMs or dedicated bare metal servers"
      },
      {
        "title": "Global Content Delivery",
        "description": "Accelerate content delivery across six continents with Vultr CDN"
      }
    ],
    "pros": [
      "Wide range of AMD and NVIDIA GPU options",
      "Extensive global network (32 data center regions)",
      "Both VM and bare metal deployment options",
      "Kubernetes and containerization support",
      "NVIDIA Preferred Cloud Partner status",
      "Simplified driver setup and licensing",
      "API and Terraform integration"
    ],
    "cons": [
      "Medium GPU availability compared to some specialized providers",
      "Less established in the GPU market compared to major hyperscalers",
      "Limited documentation specific to GPU workloads"
    ],
    "gettingStarted": [
      {
        "title": "Create an account",
        "description": "Sign up for a free Vultr account"
      },
      {
        "title": "Select GPU instance",
        "description": "Choose from AMD or NVIDIA GPU options based on your workload"
      },
      {
        "title": "Choose deployment type",
        "description": "Select between virtual machine or bare metal deployment"
      },
      {
        "title": "Configure and launch",
        "description": "Set up networking, storage, and security options before launching"
      }
    ],
    "computeServices": [
      {
        "name": "NVIDIA GPU Instances",
        "description": "Virtual machines and bare metal servers with NVIDIA GPUs",
        "instanceTypes": [
          {
            "name": "NVIDIA GH200 Grace Hopper",
            "description": "Grace Hopper Superchip for memory\u2011bound inference workloads.",
            "features": [
              "Grace CPU + H100 GPU memory coherence",
              "Strong for large\u2011context inference",
              "Datasheets and resources available"
            ]
          },
          {
            "name": "NVIDIA H100 & H200",
            "description": "Tensor Core GPUs for advanced AI, data analytics, and HPC workloads",
            "features": [
              "Unprecedented acceleration for AI workloads",
              "Suitable for complex simulations",
              "Latest Tensor Core technology"
            ]
          },
          {
            "name": "NVIDIA A100",
            "description": "Tensor Core GPU enabling scientific simulation, data analytics, and AI",
            "features": [
              "High performance for research applications",
              "Multi-instance GPU capability",
              "Versatile for diverse workloads"
            ]
          },
          {
            "name": "NVIDIA L40S",
            "description": "Combining AI compute with graphics and media acceleration",
            "features": [
              "Balanced performance for AI and graphics",
              "Suitable for media workloads",
              "Next-generation data center applications"
            ]
          },
          {
            "name": "NVIDIA A40",
            "description": "Professional graphics with powerful compute and AI capabilities",
            "features": [
              "Designed for creative and scientific challenges",
              "Balanced compute and graphics performance",
              "Professional visualization support"
            ]
          },
          {
            "name": "NVIDIA A16",
            "description": "GPU for virtual desktops and workstations",
            "features": [
              "Optimized for virtual desktop infrastructure",
              "Support for remote work solutions",
              "Efficient multi-user performance"
            ]
          },
          {
            "name": "NVIDIA H200",
            "description": "Tensor Core GPU with HBM3e memory for advanced AI.",
            "features": [
              "High\u2011throughput LLM inference/training",
              "Available across select regions",
              "Part of Vultr Cloud GPU lineup"
            ]
          }
        ]
      },
      {
        "name": "AMD GPU Instances",
        "description": "Computing infrastructure powered by AMD Instinct accelerators",
        "instanceTypes": [
          {
            "name": "AMD Instinct MI325X & MI300X",
            "description": "Setting new standards for powerful and efficient AI and HPC deployments",
            "features": [
              "Advanced accelerators for AI workloads",
              "High-performance computing capabilities",
              "Energy-efficient design"
            ]
          }
        ]
      },
      {
        "name": "Vultr Kubernetes Engine",
        "description": "Managed Kubernetes service for GPU-accelerated containerized applications",
        "features": [
          "GPU-accelerated Kubernetes clusters",
          "Global deployment options",
          "Simplified container orchestration",
          "Support for resource-intensive workloads"
        ]
      },
      {
        "name": "Vultr Serverless Inference",
        "description": "Deploy and scale GenAI models efficiently",
        "features": [
          "Quick deployment of AI models",
          "Efficient scaling",
          "Support for proprietary data or trained models",
          "Global acceleration"
        ]
      }
    ],
    "regions": "32 global cloud data center regions across North America, South America, Europe, Asia, Africa, and Australia",
    "support": "Documentation, community forums, support tickets, and dedicated customer support",
    "uniqueSellingPoints": [
      "Partnership with both AMD and NVIDIA",
      "NVIDIA Preferred Partner status",
      "Extensive global network of data centers",
      "Simplified GPU infrastructure setup",
      "Flexible deployment options (VM or bare metal)",
      "Integration with Vultr's broader cloud ecosystem"
    ]
  },
  {
    "id": "5cf897be-8662-4404-9c3b-d4938fb8fa71",
    "name": "Voltage Park",
    "slug": "voltage",
    "description": "Voltage Park provides on\u2011demand and reserved bare\u2011metal clusters with NVIDIA HGX H100 today, expanding to H200 and Blackwell (B200/B300, GB200/GB300) on term contracts.",
    "link": "https://www.voltagepark.com/",
    "docsLink": null,
    "features": [
      {
        "title": "High-Performance Hardware",
        "description": "NVIDIA HGX H100 GPUs in Dell PowerEdge XE9680 servers with 1TB RAM and v52 CPUs"
      },
      {
        "title": "Tier 3+ Data Centers",
        "description": "Six U.S.-based data centers with redundancy, availability, and reliability across power, cooling, network, and security"
      },
      {
        "title": "Blazing-Fast Network",
        "description": "NVIDIA Quantum-2 InfiniBand network with up to 3,200Gbps of aggregate bandwidth for high-speed communication"
      },
      {
        "title": "Bare Metal Access",
        "description": "Direct hardware access for maximum performance without virtualization overhead"
      },
      {
        "title": "Shadeform Integration",
        "description": "Integrated with Shadeform for console and API access"
      },
      {
        "title": "Advanced Security",
        "description": "Top-tier firewalls and comprehensive security protocols including encryption, access controls, and regular audits"
      }
    ],
    "pros": [
      "Great GPU availability for on-demand use",
      "Transparent pricing with no hidden costs",
      "Fast deployment (spin up nodes in 15 minutes)",
      "Bare metal access for maximum performance",
      "Exceptional customer service and support",
      "NVIDIA Cloud Partner status",
      "Scalable clusters from 64 to 8,176 GPUs"
    ],
    "cons": [
      "U.S.-only regions (limited global presence)",
      "Low GPU variety (primarily H100s)",
      "No persistent storage support",
      "Relatively new provider (founded 2023)"
    ],
    "gettingStarted": [
      {
        "title": "Create an account",
        "description": "Sign up for Voltage Park services through their website"
      },
      {
        "title": "Choose deployment option",
        "description": "Select between on-demand access or long-term reservation"
      },
      {
        "title": "Configure your cluster",
        "description": "Specify the number of GPUs and networking requirements"
      },
      {
        "title": "Deploy and access",
        "description": "Launch your cluster and access via SSH or preferred tools"
      }
    ],
    "computeServices": [
      {
        "name": "On-Demand GPU Access",
        "description": "Self-serve GPU instances available in minutes with no long-term contracts",
        "instanceTypes": [
          {
            "name": "NVIDIA HGX H100",
            "description": "High-performance GPU instances for AI and ML workloads",
            "features": [
              "Spin up in 15 minutes",
              "No long-term commitment required",
              "Ideal for bursts and experimentation"
            ]
          },
          {
            "name": "NVIDIA H200",
            "description": "H200 HGX nodes available on\u2011demand where capacity allows.",
            "features": [
              "HBM3e memory for faster inference",
              "Limited capacity; inquire for regions",
              "Same 15\u2011minute spin\u2011up target"
            ]
          }
        ]
      },
      {
        "name": "Reserved GPU Clusters",
        "description": "Long\u2011term GPU reservations with favorable payment terms; H100, H200, and Blackwell options.",
        "features": [
          "12+ month contracts",
          "Friendly payment terms",
          "Priority access to resources",
          "Ideal for sustained, high-priority projects"
        ]
      }
    ],
    "regions": "Six Tier 3+ designed data centers across the United States",
    "support": "Top-tier support in partnership with Penguin, helping customers set up, optimize, and scale resources effectively",
    "uniqueSellingPoints": [
      "24,000 NVIDIA H100 Tensor Core GPUs in their fleet",
      "InfiniBand clusters supporting up to 8,176 GPUs",
      "3200 Gbps of aggregate bandwidth with NVIDIA Quantum-2 InfiniBand",
      "SOC II certified infrastructure",
      "Founded by Jed McCaleb with $500M in funding"
    ]
  },
  {
    "id": "d3a46a10-53b1-4778-8995-1971f554f342",
    "name": "Massed Compute",
    "slug": "massedcompute",
    "description": "Massed Compute provides affordable on\u2011demand GPUs including H100 (SXM/NVL/PCIe) with owned hardware and transparent specs.",
    "link": "https://massedcompute.com/",
    "docsLink": "https://vm-docs.massedcompute.com/docs/intro",
    "features": [
      {
        "title": "NVIDIA Preferred Partner",
        "description": "Assurance of fully-supported, high-performance NVIDIA GPU solutions"
      },
      {
        "title": "SOC 2 Type II Compliant",
        "description": "Independent attestation of security, availability, and confidentiality controls"
      },
      {
        "title": "On-Demand & Bare-Metal Options",
        "description": "Hourly VMs or single-tenant servers\u2014no long-term contracts"
      },
      {
        "title": "Inventory API",
        "description": "REST API to list, provision, manage, and retire GPU instances programmatically"
      },
      {
        "title": "Tier III U.S. Data Centers",
        "description": "Redundant power, cooling, and network for >99.98 % uptime"
      },
      {
        "title": "Virtual Desktop Interface",
        "description": "Launch pre-configured AI/ML or VFX desktops in a browser\u2014no CLI needed"
      },
      {
        "title": "Pre-installed Drivers & Frameworks",
        "description": "CUDA, Jupyter, ComfyUI, SD, vLLM, TGI and more ready out of the box"
      }
    ],
    "pros": [
      "Broad catalog of NVIDIA GPUs from RTX A5000 up to H100 SXM5",
      "Very competitive hourly pricing with no hidden bandwidth fees",
      "Owned hardware\u2014no \u2018middle-man\u2019 latency or support hand-offs",
      "Direct access to engineers for GPU & driver questions",
      "Inventory API enables white-label and SaaS integrations"
    ],
    "cons": [
      "Data-center footprint limited to the United States",
      "No AMD GPU options at present",
      "You must maintain a prepaid credit balance to launch VMs",
      "You need to request capacity for 8-GPU H100 PCIe nodes",
      "Relatively new player compared with hyperscalers"
    ],
    "gettingStarted": [
      {
        "title": "Create an account",
        "description": "Sign up and confirm email to access the console"
      },
      {
        "title": "Add billing credits",
        "description": "Configure initial, minimum, and recharge amounts"
      },
      {
        "title": "Select a GPU template",
        "description": "Choose GPU type, quantity, and OS image from the catalog"
      },
      {
        "title": "Launch via VDI or SSH",
        "description": "Boot the VM, connect in one click, or use the API"
      }
    ],
    "computeServices": [
      {
        "name": "On-Demand GPU Instances",
        "description": "Self-service VMs billed hourly; configurable 1-, 2-, 4- or 8-GPU nodes",
        "instanceTypes": [
          {
            "name": "H100 SXM5",
            "description": "80 GB H100 SXM5 nodes ideal for frontier-scale LLM training",
            "features": [
              "Up to 8 GPUs / 640 GB vRAM",
              "126 vCPUs, 1.5 TB RAM, 10 TB NVMe",
              "$21.60 hr for 8-GPU configuration"
            ]
          },
          {
            "name": "H100 NVL",
            "description": "94 GB PCIe H100 NVL pairs optimised for inference & fine-tuning",
            "features": [
              "2, 4, or 8 GPUs per node",
              "Up to 752 GB vRAM",
              "$5.06 hr (2 \u00d7 NVL) to $20.24 hr (8 \u00d7 NVL)"
            ]
          },
          {
            "name": "A100 SXM4 / PCIe",
            "description": "80 GB A100 GPUs for mainstream training workloads",
            "features": [
              "DGX A100 or PCIe flavours",
              "From $1.20 hr (single A100 PCIe) to $9.84 hr (8 \u00d7 A100)"
            ]
          },
          {
            "name": "L40S & L40 PCIe",
            "description": "Balanced AI-and-graphics GPUs with 48 GB vRAM",
            "features": [
              "Up to 8 GPUs per node",
              "From $0.86 hr to $6.88 hr"
            ]
          },
          {
            "name": "RTX 6000 ADA / RTX A6000 / A40 / RTX A5000 / A30",
            "description": "Cost-effective options for smaller fine-tunes, VFX and rendering",
            "features": [
              "24 \u2013 48 GB vRAM",
              "Hourly rates as low as $0.25 hr"
            ]
          }
        ]
      },
      {
        "name": "Bare-Metal GPU Servers",
        "description": "Single-tenant servers with full hardware control and optional NVLink interconnects",
        "features": [
          "No virtualization overhead",
          "Customisable CPU, RAM, storage & network fabric",
          "Ideal for latency-sensitive or regulated workloads"
        ]
      },
      {
        "name": "GPU Clusters",
        "description": "Reserved multi-node clusters networked for distributed training",
        "features": [
          "Flexible interconnect (e.g. InfiniBand, 200 GbE)",
          "Long-term reservation guarantees capacity",
          "Expert performance tuning included"
        ]
      },
      {
        "name": "Inventory API",
        "description": "Programmatic access to the entire GPU catalog for SaaS or internal tooling",
        "features": [
          "List available SKUs & pricing",
          "Provision / start / stop / terminate instances",
          "Retrieve usage & billing data"
        ]
      },
      {
        "name": "On\u2011Demand GPU Pricing",
        "description": "Preconfigured VM sizes and bare\u2011metal on request.",
        "instanceTypes": [
          {
            "name": "H100 SXM5",
            "description": "8\u00d7 H100 SXM nodes.",
            "features": [
              "Quantum\u20112 style high\u2011speed fabrics",
              "Contact for reserved clusters"
            ]
          },
          {
            "name": "H100 NVL",
            "description": "94 GB NVL configurations.",
            "features": [
              "2/4/8 GPU options",
              "High VRAM per GPU"
            ]
          },
          {
            "name": "H100 PCIe",
            "description": "80 GB PCIe.",
            "features": [
              "Cost\u2011efficient single\u2011GPU options"
            ]
          }
        ]
      }
    ],
    "regions": "Tier III U.S. data centers (multiple sites in low-cost power markets)",
    "support": "Documentation portal, Discord community, email ticketing, and direct engineer chat ('Ask AI Expert')",
    "uniqueSellingPoints": [
      "Owned-and-operated NVIDIA GPU fleet\u2014no middlemen",
      "Extensive on-demand catalog ranging from A30 to H100 SXM5",
      "White-label-ready Inventory API for platform providers",
      "Browser-based virtual desktop for no-CLI launches",
      "SOC 2 Type II compliance for enterprise workloads"
    ],
    "gpuServices": [
      {
        "name": "GPU Lineup",
        "description": "NVIDIA accelerators and gaming cards.",
        "types": [
          {
            "name": "H100 SXM5",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Scale\u2011out training"
          },
          {
            "name": "H100 NVL",
            "gpuModel": "NVIDIA H100 NVL",
            "bestFor": "High\u2011VRAM workloads"
          },
          {
            "name": "H100 PCIe",
            "gpuModel": "NVIDIA H100",
            "bestFor": "General training/inference"
          }
        ]
      }
    ]
  },
  {
    "id": "973ad9bb-3d03-4c5f-b407-d90911f786bf",
    "name": "Jarvis Labs",
    "slug": "jarvis",
    "description": "JarvisLabs offers managed Jupyter/VMs with per\u2011minute billing on H200/H100/A100; strong India presence and pause/resume.",
    "link": "https://jarvislabs.ai/",
    "docsLink": "https://docs.jarvislabs.ai/",
    "features": [
      {
        "title": "Instant GPU Access",
        "description": "Get access to powerful GPUs instantly with zero setup complexity"
      },
      {
        "title": "Customizable Environments",
        "description": "Wide range of pre-configured templates for various AI/ML frameworks"
      },
      {
        "title": "Pay-as-you-go",
        "description": "Flexible pricing model with no upfront commitments"
      },
      {
        "title": "Enterprise-grade Infrastructure",
        "description": "Reliable and scalable infrastructure for production workloads"
      },
      {
        "title": "Instance Management",
        "description": "Easy pause, resume, and delete functionality for cost optimization"
      },
      {
        "title": "SSH Access",
        "description": "Full SSH access to instances for complete control"
      },
      {
        "title": "GPU Switching",
        "description": "Switch GPU types when resuming paused instances"
      },
      {
        "title": "Regional Flexibility",
        "description": "Resume instances within the same region for seamless operation"
      }
    ],
    "pros": [
      "Quick setup - get started in less than 5 minutes",
      "Easy instance lifecycle management (pause, resume, delete)",
      "Ability to switch GPU types when resuming instances",
      "Wide range of pre-configured templates",
      "Cost optimization through pause/resume functionality",
      "Enterprise-grade infrastructure with pay-as-you-go pricing"
    ],
    "cons": [
      "Limited information available about specific GPU types",
      "Regional availability details not clearly specified",
      "Newer player compared to established cloud providers"
    ],
    "gettingStarted": [
      {
        "title": "Create a New Account",
        "description": "Sign up for a new account on the Jarvis Labs platform"
      },
      {
        "title": "Recharge Your Wallet",
        "description": "Add funds to your account in the Recharge section"
      },
      {
        "title": "Choose and Launch a Template",
        "description": "Explore the wide range of templates, configure your desired setup, and click Launch"
      },
      {
        "title": "Manage Your Instances",
        "description": "Easily pause \u23f8\ufe0f, resume \u25b6\ufe0f, and delete \ud83d\uddd1\ufe0f your instances with just a few clicks"
      }
    ],
    "computeServices": [
      {
        "name": "GPU Instances",
        "description": "On-demand GPU instances with various configuration options",
        "features": [
          "Pre-configured templates for popular ML frameworks",
          "Scalable GPU resources",
          "Instance pause and resume capabilities",
          "SSH access for full control"
        ]
      },
      {
        "name": "Managed Workbenches",
        "description": "Spin up notebooks or VMs with NVIDIA GPUs.",
        "instanceTypes": [
          {
            "name": "H200",
            "description": "On\u2011demand H200 for large models.",
            "features": [
              "Minute billing",
              "Fast spin\u2011up"
            ]
          },
          {
            "name": "H100",
            "description": "On\u2011demand H100.",
            "features": [
              "Starting near ~$2.99/GPU\u2011hr in public materials",
              "Pause/resume to cut idle"
            ]
          }
        ]
      }
    ],
    "regions": "Multiple regions available with flexibility to resume instances within the same region",
    "support": "Documentation, tutorials, and getting started guides available",
    "uniqueSellingPoints": [
      "5-minute setup process for immediate productivity",
      "Flexible instance management with pause/resume functionality",
      "GPU type switching capability when resuming instances",
      "Wide selection of pre-configured AI/ML templates"
    ],
    "gpuServices": [
      {
        "name": "GPU Options",
        "description": "NVIDIA lineup for AI/ML.",
        "types": [
          {
            "name": "H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Large\u2011context serving"
          },
          {
            "name": "H100",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training & inference"
          },
          {
            "name": "A100 40/80",
            "gpuModel": "NVIDIA A100",
            "bestFor": "Training"
          }
        ]
      }
    ]
  },
  {
    "id": "94678cad-14ab-4bd7-9fcb-72d15ee11ce7",
    "name": "IO.NET",
    "slug": "ionet",
    "description": "io.net is a decentralized GPU network offering H100 (PCIe/SXM) and other GPUs via marketplace pricing, with significant cost differences vs hyperscalers.",
    "link": "https://io.net/",
    "docsLink": "https://docs.io.net/",
    "features": [
      {
        "title": "Massive Decentralized Network",
        "description": "Access to 300,000+ verified GPUs from 139 countries with 6,000+ cluster-ready GPUs"
      },
      {
        "title": "Rapid Deployment",
        "description": "Deploy clusters in under 90 seconds with auto-scaling capabilities"
      },
      {
        "title": "Multiple Deployment Options",
        "description": "Choose from containers, Ray clusters, or bare metal based on workload needs"
      },
      {
        "title": "Built on Ray.io",
        "description": "Uses the same distributed computing framework that OpenAI used to train GPT-3"
      },
      {
        "title": "IO Intelligence",
        "description": "AI models, smart agents, and API integration for workflow automation"
      },
      {
        "title": "Mesh VPN Security",
        "description": "Kernel-level VPN with secure mesh protocols for data protection"
      },
      {
        "title": "Flexible Pricing",
        "description": "Pay with $IO tokens, no long-term contracts or complex KYC requirements"
      }
    ],
    "pros": [
      "Up to 90% cost savings compared to AWS, GCP, and Azure",
      "Fastest deployment time in the industry (under 90 seconds)",
      "Massive global network with 300,000+ GPUs available",
      "No waitlists, approvals, or long-term contracts required",
      "Built on proven Ray.io framework used by OpenAI",
      "Wide range of GPU types from consumer to enterprise grade",
      "Auto-scaling and dynamic resource allocation"
    ],
    "cons": [
      "Newer platform compared to established cloud providers",
      "Decentralized nature may have performance consistency variations",
      "Primarily crypto-native payment model ($IO tokens)",
      "Less comprehensive documentation compared to major cloud providers",
      "Performance depends on distributed node quality and connectivity"
    ],
    "gettingStarted": [
      {
        "title": "Sign up for IO.NET",
        "description": "Create an account on the IO.NET platform with no complex KYC requirements"
      },
      {
        "title": "Acquire $IO tokens",
        "description": "Purchase $IO tokens for compute payments or add other supported payment methods"
      },
      {
        "title": "Choose deployment type",
        "description": "Select from containers, Ray clusters, or bare metal based on your workload"
      },
      {
        "title": "Configure cluster",
        "description": "Specify GPU requirements, region preferences, and scaling options"
      },
      {
        "title": "Deploy in seconds",
        "description": "Launch your cluster in under 90 seconds and start your AI/ML workloads"
      }
    ],
    "computeServices": [
      {
        "name": "IO Cloud",
        "description": "On-demand GPU clusters for AI/ML workloads with multiple deployment options",
        "instanceTypes": [
          {
            "name": "Container as a Service",
            "description": "Containerized deployments with familiar tooling and native support",
            "features": [
              "Docker container support",
              "Pre-configured AI/ML environments",
              "Scalable from single GPU to massive clusters"
            ]
          },
          {
            "name": "Ray Clusters",
            "description": "One-line deployment for ML workloads with pre-configured environments",
            "features": [
              "Built on Ray.io distributed computing framework",
              "Support for PyTorch, TensorFlow, and other ML frameworks",
              "Optimized for distributed training and inference"
            ]
          },
          {
            "name": "Bare Metal",
            "description": "Direct GPU access with root-level control for maximum performance",
            "features": [
              "Full hardware control",
              "No virtualization overhead",
              "Custom configuration options"
            ]
          }
        ]
      },
      {
        "name": "IO Intelligence",
        "description": "AI models, smart agents, and API integration platform",
        "features": [
          "Custom AI model deployment",
          "Intelligent agent framework",
          "Easy API integration for workflows",
          "Automated decision-making capabilities"
        ]
      },
      {
        "name": "Marketplace",
        "description": "Decentralized pool of GPU providers with unified APIs.",
        "instanceTypes": [
          {
            "name": "H100 PCIe",
            "description": "Marketplace H100 PCIe offers.",
            "features": [
              "Explorer lists around ~$0.89\u2013$1.70/GPU\u2011hr tiers",
              "Varies by supplier and staking tier"
            ]
          },
          {
            "name": "H100 SXM5",
            "description": "Marketplace H100 SXM offers.",
            "features": [
              "Explorer shows around ~$1.19\u2013$1.99/GPU\u2011hr",
              "Varies by region"
            ]
          }
        ]
      }
    ],
    "gpuServices": [
      {
        "name": "Consumer GPUs",
        "description": "Cost-effective consumer-grade GPUs for development and testing",
        "types": [
          {
            "name": "GeForce RTX 4090",
            "gpuModel": "NVIDIA GeForce RTX 4090",
            "bestFor": "AI development, gaming workloads, content creation"
          }
        ]
      },
      {
        "name": "Professional GPUs",
        "description": "High-performance professional GPUs for production workloads",
        "types": [
          {
            "name": "H100 PCIe",
            "gpuModel": "NVIDIA H100 PCIe",
            "bestFor": "Large-scale AI training, enterprise ML workloads"
          },
          {
            "name": "H100 SXM5",
            "gpuModel": "NVIDIA H100 SXM5",
            "bestFor": "High-bandwidth AI training, distributed computing"
          },
          {
            "name": "H200",
            "gpuModel": "NVIDIA H200",
            "bestFor": "Next-generation AI workloads, large language models"
          }
        ]
      },
      {
        "name": "GPU Catalog",
        "description": "Range spans consumer to datacenter GPUs.",
        "types": [
          {
            "name": "H100 PCIe",
            "gpuModel": "NVIDIA H100",
            "bestFor": "Training/inference"
          },
          {
            "name": "H100 SXM5",
            "gpuModel": "NVIDIA H100 SXM5",
            "bestFor": "Higher interconnect BW; training"
          }
        ]
      }
    ],
    "pricingOptions": [
      {
        "name": "Ray Cluster Pricing",
        "description": "Most cost-effective option for distributed ML workloads using Ray framework"
      },
      {
        "name": "Container Pricing",
        "description": "Standard containerized deployments with Docker support"
      },
      {
        "name": "Bare Metal Pricing",
        "description": "Premium pricing for direct hardware access and maximum performance"
      },
      {
        "name": "Auto-scaling",
        "description": "Dynamic pricing based on actual resource usage with automatic scaling"
      }
    ],
    "regions": "Global distributed network across 139 countries with intelligent geographic clustering and latency optimization",
    "support": "Documentation portal, Discord community (500,000+ members), Telegram support, and direct engineering support for GPU and driver questions",
    "uniqueSellingPoints": [
      "Largest decentralized GPU network with 300,000+ verified GPUs",
      "Industry-leading deployment speed (under 90 seconds)",
      "Up to 90% cost savings compared to traditional cloud providers",
      "Built on Ray.io framework used by OpenAI for GPT-3 training",
      "No waitlists, approvals, or long-term contracts required",
      "Native $IO token economy with staking and rewards",
      "Proof of Time-Lock verification system for guaranteed compute availability"
    ]
  }
]