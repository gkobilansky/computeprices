{
  "id": "a4c4b4ea-4de7-4e04-8d40-d4c4fc1d8182",
  "name": "Fluidstack",
  "description": "Fluidstack provides on‑demand GPUs from H200/H100 to A100 and L40S, plus private clusters on request.",
  "pros": [
    "Highly cost-effective (30-80% lower costs compared to major cloud providers)",
    "Large-scale GPU availability (10,000+ NVIDIA H100 GPUs deployed)",
    "Rapid deployment and scaling capabilities",
    "Fully managed infrastructure with 24/7 support",
    "Flexible options from on-demand instances to reserved clusters",
    "MLOps services included at no extra cost",
    "Wide range of GPU models available",
    "Enterprise-grade security and compliance"
  ],
  "cons": [
    "Relatively newer and smaller compared to major cloud providers",
    "Primary focus on AI and ML workloads may not suit all use cases",
    "Limited global presence compared to hyperscalers",
    "Less established brand recognition in the broader cloud market"
  ],
  "link": "https://www.fluidstack.io/",
  "slug": "fluidstack",
  "computeServices": [
    {
      "name": "GPU Instances",
      "description": "On‑demand dedicated GPUs for AI workloads with competitive pricing.",
      "instanceTypes": [
        {
          "name": "H200 SXM",
          "description": "141 GB HBM3e for memory-intensive workloads.",
          "features": [
            "Latest Hopper architecture with HBM3e",
            "Available in multiple regions by request"
          ]
        },
        {
          "name": "H100 SXM",
          "description": "80 GB HBM for high-performance training.",
          "features": [
            "Proven Hopper architecture",
            "Cluster options for multi-node workloads"
          ]
        }
      ]
    }
  ],
  "gpuServices": [
    {
      "name": "GPU Lineup",
      "description": "Range of NVIDIA GPUs.",
      "types": [
        {
          "name": "H200 SXM",
          "gpuModel": "NVIDIA H200",
          "bestFor": "Training/inference"
        },
        {
          "name": "H100 SXM",
          "gpuModel": "NVIDIA H100",
          "bestFor": "Training/inference"
        },
        {
          "name": "A100 80GB",
          "gpuModel": "NVIDIA A100",
          "bestFor": "Training/economical"
        },
        {
          "name": "L40S",
          "gpuModel": "NVIDIA L40S",
          "bestFor": "Inference & media"
        }
      ]
    }
  ],
  "hqCountry": "GB",
  "tagline": "Distributed GPU cloud with flexible pricing",
  "tags": [
    "Budget"
  ],
  "category": "Rapidly-catching neocloud"
}
