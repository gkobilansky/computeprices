{
  "id": "0075753d-ed17-4904-b7e4-4e61849b252c",
  "name": "Deep Infra",
  "slug": "deep-infra",
  "description": "Deep Infra offers serverless AI APIs and dedicated GPU rentals with fast SSH access and low hourly pricing across flagship NVIDIA accelerators.",
  "link": "https://deepinfra.com",
  "docsLink": "https://deepinfra.com/docs",
  "features": [
    {
      "title": "Serverless Model APIs",
      "description": "OpenAI-compatible endpoints for 100+ models with autoscaling and pay-per-token billing"
    },
    {
      "title": "Dedicated GPU Rentals",
      "description": "B200 instances with SSH access spin up in about 10 seconds and bill hourly"
    },
    {
      "title": "Custom LLM Deployments",
      "description": "Deploy your own Hugging Face models onto dedicated A100, H100, H200, or B200 GPUs"
    },
    {
      "title": "Transparent GPU Pricing",
      "description": "Published per-GPU hourly rates for A100, H100, H200, and B200 with competitive pricing"
    },
    {
      "title": "Inference-Optimized Hardware",
      "description": "All hosted models run on H100 or A100 hardware tuned for low latency"
    }
  ],
  "pros": [
    "Simple OpenAI-compatible API alongside controllable GPU rentals",
    "Competitive hourly rates for flagship NVIDIA GPUs including latest B200",
    "Fast provisioning with SSH access for dedicated instances (ready in ~10 seconds)",
    "Supports custom deployments in addition to hosted public models"
  ],
  "cons": [
    "Region list is not clearly published in the public marketing pages",
    "Primarily focused on inference and GPU rentals rather than broader cloud services",
    "Newer player compared to established cloud providers"
  ],
  "gettingStarted": [
    {
      "title": "Create an account",
      "description": "Sign up (GitHub-supported) and open the Deep Infra dashboard"
    },
    {
      "title": "Enable billing",
      "description": "Add a payment method to unlock GPU rentals and API usage"
    },
    {
      "title": "Pick a GPU option",
      "description": "Choose serverless APIs or dedicated A100, H100, H200, or B200 instances"
    },
    {
      "title": "Launch and connect",
      "description": "Start instances with SSH access or call the OpenAI-compatible API endpoints"
    },
    {
      "title": "Monitor usage",
      "description": "Track spend and instance status from the dashboard and shut down when idle"
    }
  ],
  "computeServices": [
    {
      "name": "Serverless Inference",
      "description": "Hosted model APIs with autoscaling on H100/A100 hardware.",
      "features": [
        "OpenAI-compatible REST API surface",
        "Runs 100+ public models with pay-per-token pricing",
        "Autoscaling for low latency without manual instance management"
      ]
    },
    {
      "name": "Dedicated GPU Instances",
      "description": "On-demand GPU nodes with SSH access for custom workloads.",
      "instanceTypes": [
        {
          "name": "B200",
          "description": "180 GB HBM Blackwell GPUs for premium training and inference.",
          "features": [
            "SSH access ready in about 10 seconds",
            "Latest generation NVIDIA Blackwell architecture"
          ]
        },
        {
          "name": "H200",
          "description": "141 GB HBM GPUs tuned for memory-heavy LLMs and serve workloads.",
          "features": [
            "Extended memory capacity for long-context models",
            "Fast provisioning with SSH access"
          ]
        },
        {
          "name": "H100",
          "description": "80 GB Hopper GPUs for high-performance training and inference.",
          "features": [
            "Optimized for transformer-based models",
            "Fast provisioning with SSH access"
          ]
        },
        {
          "name": "A100",
          "description": "80 GB A100 GPUs for cost-effective training and fine-tuning.",
          "features": [
            "Proven architecture for a wide range of workloads",
            "Competitive pricing for budget-conscious deployments"
          ]
        }
      ]
    }
  ],
  "gpuServices": [
    {
      "name": "GPU Lineup",
      "description": "Dedicated GPUs for custom deployments and rentals.",
      "types": [
        {
          "name": "B200",
          "gpuModel": "NVIDIA B200",
          "bestFor": "Frontier-scale training or high-throughput inference with large memory headroom"
        },
        {
          "name": "H200",
          "gpuModel": "NVIDIA H200",
          "bestFor": "Long-context LLM serving and memory-bound workloads"
        },
        {
          "name": "H100",
          "gpuModel": "NVIDIA H100",
          "bestFor": "High-performance training and low-latency inference"
        },
        {
          "name": "A100",
          "gpuModel": "NVIDIA A100",
          "bestFor": "Cost-effective training, fine-tuning, and inference"
        }
      ]
    }
  ],
  "pricingOptions": [
    {
      "name": "Serverless pay-per-token",
      "description": "OpenAI-compatible inference APIs with pay-per-request billing on H100/A100 hardware"
    },
    {
      "name": "Dedicated GPU hourly rates",
      "description": "Published transparent hourly pricing for A100, H100, H200, and B200 GPUs with pay-as-you-go billing"
    },
    {
      "name": "No long-term commitments",
      "description": "Flexible hourly billing for dedicated instances with no prepayments or contracts required"
    }
  ],
  "regions": "Region list not published on the GPU Instances page; promo mentions Nebraska availability alongside multi-region autoscaling messaging.",
  "support": "Documentation site, dashboard guidance, Discord community link, and contact-sales options.",
  "uniqueSellingPoints": [
    "Combines OpenAI-compatible serverless APIs with controllable GPU rentals that boot in about 10 seconds",
    "Competitive pricing for flagship NVIDIA GPUs including latest B200 Blackwell architecture",
    "SSH-accessible dedicated instances plus custom model deployments for full control"
  ],
  "hqCountry": "US",
  "tagline": "Optimized inference for open-source models",
  "tags": [],
  "category": "Rapidly-catching neocloud"
}
